{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotions in Conflicts: Data Collection, Analysis & Visualization\n",
    "\n",
    "\n",
    "In this notebook we present our pipeline to answer the research questions that we raised in Milestone 1. We start by a part 0, where we present and discuss some general points, mostly technical, that we encountered and considered throughout the project. Then, in the first part, we show our step-step strategy on how we proceed to answer the individual questions. In the second part, the code we used to fetch the data and to do the analysis is provided. In the third part we present our visualizations of the data analysis.\n",
    "\n",
    "In this notebook, the code in Part 2 is executed on a sample of nine datafiles fetched from the cluster (3 for each of the 3 schemas (gkg, mentions, export)) and intermediate outputs are shown to guide the reader through the data processing part. The code used to fetch and process the whole GDELT dataset in the cluster is found in the `src` folder and the individual files are hyperlinked in this notebook. The processing of the extracted cluster data is shown on the actual fetched data.\n",
    "\n",
    "The statistical analysis and the visualization part is as well done on the data gathered from the whole GDELT dataset, and the detailed interpretations are presented in our [data story](https://matterhorn-ada.github.io)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Part 0](#Part0): General points\n",
    "\n",
    "\n",
    "- [Part 1](#Part1): Strategy to answer the research questions\n",
    "    - [Question 0](#Part1_Q0): Exploratory analysis: Where does the news come from?\n",
    "    - [Question 1](#Part1_Q1): Are we emotionally biased? \n",
    "    - [Question 2](#Part1_Q2): Are some countries ignored in the news?\n",
    "    - [Question 3](#Part1_Q3): Are we emotionally predictable?\n",
    "    - [Question 4](#Part1_Q4): Do we have a saturation limit?\n",
    "    - [Question 5](#Part1_Q5): Who is more emotional?\n",
    "    \n",
    "    \n",
    "- [Part 2](#Part2): Code\n",
    "    - [Question 0](#Part2_Q0): Exploratory Data Fetching \n",
    "    - [Question 1](#Part2_Q1): Fetching & preprocessing of the data   \n",
    "    - [Question 2](#Part2_Q2): Aggregating \n",
    "    - [Question 3](#Part2_Q3a): Fetching & preprocessing of the data (Cluster Code)\n",
    "    - [Question 3](#Part2_Q3b): Model building (learning & validating) \n",
    "    - [Question 4](#Part2_Q4): V2Tone and Emotion Metric\n",
    "    - [Question 5](#Part2_Q5): GCAM Emotions and Emotions dictionary\n",
    "    \n",
    "    \n",
    "- [Part 3](#Part3): Statistics & Visualization\n",
    "    - [Question 0](#Part3_Q0): Choropleth maps\n",
    "    - [Question 1](#Part3_Q1): Dependency evaluation\n",
    "    - [Question 2](#Part3_Q2): Dependency evaluation\n",
    "    - [Question 4](#Part3_Q4): Emotion Saturation\n",
    "    - [Question 5](#Part3_Q5): GCAM words and countries\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0 <a id='Part0'></a>\n",
    "\n",
    "- All the filtering and aggregation steps of processing the data were executed in the cluster and the resulting dataset were exported as either `parquet` or `json` files. The later processing was done with `Pyspark` and `Pandas` dataframes. To handle the large dataset locally, e.g. for the plots and model creation, some downsampling was done. For the plot, random sampling was performed, and to generate the model data, stratified sampling was performed to garantuee a similar amount of data in each feature category.\n",
    "\n",
    "\n",
    "- When working with country names and country codes from different data sources (e.g. GDELT country codes, GeoJSON country codes, United Nations country names, ...) several heterogeneities in the names and codes were encountered. GDELT is using the [FIPS country codes](https://en.wikipedia.org/wiki/List_of_FIPS_country_codes) which is mainly used by the US government, whereas the [alpha-2](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-22) letter code introduced International Organization for Standardization is more widely used (example: FIPS code for Switzerland is SZ, and the alpha-2 code is CH). For the processing, we converted the GDELT code to the standard by creating a [dictionary](../data/CountryCodes/CountryCode_FIPS_alpha.csv), with the data taken from [geodatasource.com](https://www.geodatasource.com/resources/tutorials/international-country-code-fips-versus-iso-3166/), however some manuel work had to be done for the country names (some external dataset used did not provide the country code and thus had to be joined on the country name, e.g. the [Human development index](http://hdr.undp.org/en/content/human-development-index-hdi)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 <a id='Part1'></a>\n",
    "\n",
    "### Pursued strategy to answer the research questions:\n",
    "\n",
    "Many of our question make use of an emotion charge defined by us, so it makes sense to get familiar with it from the beginning\n",
    "\n",
    "**Emotion Charge** *Our emotion metrics explained*\n",
    "\n",
    "To make a reliable and accurate metrics that relates the emotion to each event and then to each country we did the following:\n",
    "1. Get the absolute value of the difference between the Average Tone and the Polarity which is provided by GDELT and which rely on the word count of positive and negative words. Why do we do that: because we can have really polar speeches that translate into a big emotional charge, however the average tone is not capable of transmiting that as it is the difference between the negative and positive tone of the speech. Having a max positive and negative score gives an avg tone of 0, 100 - 100 = 0, however the speech was highly emotional. Taking the absolute value we can see that as it goes further away from 0 the more emotional it is.\n",
    "    \n",
    "2. Then we normalize the word count to map it to a value between 0 and 1. You wonder why: well, the more words the GDELT score was based on to give the polarity and the AvgTone scores, the more reliable is the result. Therefore, normalizing the word count and then multiplying the value we get from step 1 by this value let us have a notion of the weight of the value of step 1 regarding the final sum.\n",
    "    \n",
    "3. After we sum all the values we get from step 2 when doing the group by operation by country, and we divide the value we obtain by the number of events that country had in that month. Now you probably won't ask why as the reason is similar to step 2. If there is a country with a loooot of events (as it is the case for the USA), the final sum is biased, USA might not be the most emotional country but as it has the most events recorded by the GDELT it will appear so. So by dividing by the number of events we get the average emotional charge for the speeches of that country.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 0 <a id='Part1_Q0'></a>\n",
    "\n",
    "**Exploratory analysis: Where does the news come from?**\n",
    "\n",
    "#### Fetching & Processing the data \n",
    "\n",
    " From the GDELT dataset we fetch the following information from the \"Mentions\" and \"Events\" sets:\n",
    "\n",
    "- Url of the article mentioning the source \n",
    "- Location of the event\n",
    "- Location of the country reporting the news: \n",
    "    - Get the country reporting the news from the url of the article using the [GDELT Geographic Source Lookup](https://blog.gdeltproject.org/mapping-the-media-a-geographic-lookup-of-gdelts-sources/)\n",
    "\n",
    "#### Analysis\n",
    "\n",
    "- Where do the events happen?\n",
    "    - Group by the countries and count the number of events happening in this country.\n",
    "\n",
    "- Who reports the news?\n",
    "    - Group by the countries reporting the news and count the number of events reported by each country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 <a id='Part1_Q1'></a>\n",
    "\n",
    "**Are we emotionally biased?** Do the number of conflicts or their distance from our home define our emotions? \n",
    "\n",
    "#### Fetching & Processing the data \n",
    "\n",
    " From the GDELT dataset we fetch the following information from the \"Mentions\", \"Events\" and \"Global Knowledge Graph\" sets:\n",
    "\n",
    "- Url of the article mentioning the source \n",
    "- Average Tone (Event-file), Polarity, Tone (gkg-file)\n",
    "- Location of the event (latitudinal and longitudinal coordinates)\n",
    "- Number of times the event is mentioned in the news (NumArticles) \n",
    "- Calculation of the distance between the source article and the event: \n",
    "    1. Get the country from the url of the article using the [GDELT Geographic Source Lookup](https://blog.gdeltproject.org/mapping-the-media-a-geographic-lookup-of-gdelts-sources/)\n",
    "    2. Get the geographic coordinates of the capital of the source country using the csv file provided by [this website](http://techslides.com/list-of-countries-and-capitals)\n",
    "    3. Calculate the geographic distance between the source article and the event with the [Great-Circle distance formula](https://en.wikipedia.org/wiki/Great-circle_distance)\n",
    "    \n",
    "    Note that in this approach we approximate the location of the source reporting the article by the capital of the country reporting the news.\n",
    "\n",
    "#### Analysis\n",
    "\n",
    "- Evaluation of the dependency between the emotions and the distance: \n",
    "    1. Plot the emotion metrics (Average Tone/ Emotional charge (measure of polarity and tone)) against the distance for each event. Distinguish between the location of the event (colour codes) to see if the country where the event occurred influences the emotion, or the other way around, whether some news from a given country only get reported if they have a negative or positive association. \n",
    "    \n",
    "\n",
    "- Evaluation of the dependency between the emotions and the importance of the conflict:\n",
    "    1. Evaluation whether there is a dependency of the emotion metrics and the importance of the conflict using an \"importance of an event\" metrics provided by GDELT (3 very similar metrics are given, NumMentions, NumSources, NumArticles, and we decided to use NumArticles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 <a id='Part1_Q2'></a>\n",
    "\n",
    "**Are some countries ignored in the news?**  Is the number of conflicts taking place in a country in relation with the number of mentions in the media depending on where the conflict has happened? \n",
    "\n",
    "#### Fetching the data \n",
    "\n",
    "We use the same dataset than in Question 1, in addition to the dataset [\"population by country\"](https://en.wikipedia.org/wiki/List_of_countries_by_population_(United_Nations)) provided by the United Nations (2017).\n",
    "\n",
    "#### Analysis\n",
    "\n",
    "- Group by the event country and sum up the number of conflicts and the number of mentions \n",
    "- Evaluate whether there is a correlation between the number of conflicts and mentions or not\n",
    "- Identify countries with a high number of conflicts, but a comparewise low number of mentions\n",
    "- Identify whether the number of people living in a country influence the number of event in a country\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 <a id='Part1_Q3'></a>\n",
    "\n",
    "**Are we emotionally predictable?** Can we observe patterns of emotions with respect to a country, religion or an ethnical group? Can we derive a model predicting emotions in case of a new conflict based on its specific features?\n",
    "\n",
    "We derive a model using the tone metrics given by GDELT as the response variable and the following variables as input features:\n",
    "\n",
    "Features are: CountryEvent, CountrySource, EventType, NumPeople, ActorReligion, HDI  \n",
    "\n",
    "CountryEvent: Geographic country where the event has happened.\n",
    "\n",
    "CountrySource: Geographic country reporting the news article.\n",
    "\n",
    "EventType: This reflects the type of the event. Categorical value with the following values: AFFECT, ARREST, KIDNAP, KILL, PROTEST, SEIZE, WOUND, etc. \n",
    "\n",
    "NumPeople: Number of people concerned by the EventType. \n",
    "\n",
    "ActorReligion: Religion of the actors implied in the event and indicated by the CAMEO Religious Coding Scheme (see Chapter 4 from the [CAMEO\n",
    "Conflict and Mediation Event Observations Event and Actor Codebook](http://data.gdeltproject.org/documentation/CAMEO.Manual.1.1b3.pdf)). 19 religions are reported (categorical variable).\n",
    "\n",
    "HDI: Human Development Index. This index takes into account the development of a country, not  only including its economic growth (measured by the gross national income), but also life expectance and education (more information are found on the [United Nations Development Website](http://hdr.undp.org/en/content/human-development-index-hdi). Categorical variable (HDI $\\in$ [very high, high, medium, low]). We use the [latest version](http://hdr.undp.org/en/composite/HDI) (released in Sep. 2018) covering the period of 2017. \n",
    "\n",
    "Tone: The score ranges from -100 (extremely negative) to +100 (extremely positive). Common values range between -10 and +10, with 0 indicating neutral. We categorize this variable into 5 categories ([-100, -10], [-10, -5], [-5, 0], [0, 5], [5,100])\n",
    "\n",
    "#### Analysis\n",
    "\n",
    "- We fetch the required data from GDELT and we do a stratified downsampling on the EventType category (since the different classes do not have equal data points)\n",
    "- We split the data into a training and test set train a machine model algorithm (either decision tree, or random forest) on the training set and we test the accuracy on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 <a id='Part1_Q4'></a>\n",
    "\n",
    "**Do we have a saturation limit?** Does increasing number of conflicts make people feel worse and worse or is there some limit?\n",
    "\n",
    "#### Fetching the data \n",
    "\n",
    "For question 4 the way we proceeded was the following:\n",
    "- Read files by month.\n",
    "- Get a dataframe where we can associate the gkg GCAM and V2Tone with the country by the url in the mentions dataframe.\n",
    "- Calculate our metrics of Emotion_Charge from the V2Tone data.\n",
    "- Get a final dataframe where we group by country summing the Emotion_Charge of the individual events of that country while also counting how many events happened in that country in that month. \n",
    "\n",
    "\n",
    "#### Analysis\n",
    "\n",
    "  - Calculation of the increasing number of conflicts: \n",
    "      1. Get the location of the country\n",
    "      2. Parse through the gkg files (in the time interval we wish) and get the events referent to a country.\n",
    "      3. Associate quantity of conflicts with emotions passed\n",
    "\n",
    "  - Possible limit of the emotions: \n",
    "      1. Get the V2Tone and the GCAM feelings referent to the events\n",
    "      2. Evaluate the emotions that we have for each of these events, observing how the media shows the events and if there are some insensibility (measured by lower V2Tone scores and more neutral GCAM feelings) or not after a threshold number of events.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 <a id='Part1_Q5'></a>\n",
    "\n",
    "**Who is more emotional and what do they say?** Lets go deeper into question 4, this time associating words with countries. How are some countries clustered referent to the emotions they represent (GCAM data). \n",
    "\n",
    "#### Fetching the data \n",
    "\n",
    "From the GDELT dataset we fetch the following information from the \"GKG\" and \"Events\":\n",
    "\n",
    "  - Locations\n",
    "  - GCAM words\n",
    "\n",
    "#### Analysis\n",
    "\n",
    "\n",
    "  - Relation between the country and the emotion: \n",
    "      1. We get the GCAM data and with the mentions dataset we associate the words of GCAM to the countries.\n",
    "      2. We associate the word code to the word itself, and make use of our own almost 800 long list of human readable words, so we get a list of words per country that we can understand.\n",
    "      3. We see the most often words used by specific countries.\n",
    "      4. We perform a principal component analysis to see if some countries cluster together due to similar choice of words in the news article reports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 <a id='Part2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# regular imports\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import radians, sqrt, sin, cos, atan2\n",
    "import pickle\n",
    "\n",
    "# function imports\n",
    "from Schema import *\n",
    "from Visualization import *\n",
    "from helper_functions import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#import findspark\n",
    "#findspark.init('C:\\opt\\spark')\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# scikit learn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import tree\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# update when changing functions\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_DIR = 'hdfs:///datasets/gdeltv2'  # cluster code\n",
    "DATA_DIR = '../data/Gdelt/' # local code\n",
    "\n",
    "# directory for local files \n",
    "DATA_LOCAL = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open GDELT data\n",
    "gkg_df = spark.read.option(\"sep\", \"\\t\").csv(os.path.join(DATA_DIR, \"*.gkg.csv\"),schema=GKG_SCHEMA)\n",
    "events_df = spark.read.option(\"sep\", \"\\t\").csv(os.path.join(DATA_DIR, \"*.export.CSV\"),schema=EVENTS_SCHEMA)\n",
    "mentions_df = spark.read.option(\"sep\", \"\\t\").csv(os.path.join(DATA_DIR, \"*.mentions.CSV\"),schema=MENTIONS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open helper datasets\n",
    "CountryToCapital = spark.read.format(\"csv\").option(\"header\", \"true\").load(DATA_LOCAL + \"CountryInformations/country-capitals.csv\")\n",
    "Domains = spark.read.format(\"csv\").option(\"header\", \"true\").load(DATA_LOCAL + \"UrlToCountry/urls.csv\")\n",
    "Domains = Domains.select(Domains['alpha-2'].alias('code') ,Domains['name'].alias('country_source'), 'region', Domains['web'].alias('url'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+------+-----------------+\n",
      "|code|country_source|region|              url|\n",
      "+----+--------------+------+-----------------+\n",
      "|  AF|   Afghanistan|  Asia| 108worldnews.com|\n",
      "|  AF|   Afghanistan|  Asia|           1tv.af|\n",
      "|  AF|   Afghanistan|  Asia|       1tvnews.af|\n",
      "|  AF|   Afghanistan|  Asia|       aff.org.af|\n",
      "|  AF|   Afghanistan|  Asia|afghan-review.com|\n",
      "+----+--------------+------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Domains.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "urls linking the url of the news article to the country which is reporting the event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q0.  Exploratory Data Fetching <a id='Part2_Q0'></a>\n",
    "The whole cluster code is found [here](FetchingData/Q0_cluster.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole news sources urls processing code is found [here](domains.py). The biggest issue we have faced was probably the different notations of countries - different names, different country codes - so we had to convert inbetween them to be able to work and visualize all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "|ActionGeo_CountryCode|count|\n",
      "+---------------------+-----+\n",
      "|                   CI|   15|\n",
      "|                   FI|    1|\n",
      "|                   IC|    1|\n",
      "|                   RO|    9|\n",
      "|                   SL|    7|\n",
      "+---------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### get the number of events grouped by the country they occurred\n",
    "\n",
    "# select the required data from Event Dataset\n",
    "events_1 = events_df.select('ActionGeo_CountryCode').filter(events_df.ActionGeo_CountryCode.isNotNull())\n",
    "CountryEvent_number = events_1.groupBy('ActionGeo_CountryCode').count()\n",
    "CountryEvent_number.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|     country_source|count|\n",
      "+-------------------+-----+\n",
      "|Korea (Republic of)|   36|\n",
      "|        Philippines|  166|\n",
      "|           Malaysia|  151|\n",
      "|               Fiji|    5|\n",
      "|             Turkey|   40|\n",
      "+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### get the number of events grouped by the country reporting\n",
    "\n",
    "# select the required data from Mentions Dataset\n",
    "mentions_1 = mentions_df.select(\"GLOBALEVENTID\", \"MentionType\", \"MentionSourceName\") \\\n",
    "                .filter(mentions_df[\"MentionType\"] == '1') # MentionType 1 refers to the web articles and not e.g. books\n",
    "\n",
    "# join the domain df with the country corresponding to the source\n",
    "mentions_2 = mentions_1.join(Domains, Domains['url'] == mentions_1['MentionSourceName'], \"left_outer\")\n",
    "\n",
    "# filter out urls that are unknown\n",
    "mentions_3 = mentions_2.filter(mentions_2.country_source.isNotNull())\n",
    "\n",
    "events_1 = events_df.select('GLOBALEVENTID')\n",
    "\n",
    "mention_event = events_1.join(mentions_3, 'GLOBALEVENTID')\n",
    "CountrySource_number = mention_event.groupBy('country_source').count()\n",
    "CountrySource_number.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.  Are we emotionally biased?   <a id='Part2_Q1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole cluster code is found in the [Q1_cluster.py](FetchingData/Q1_cluster.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unknown urls:  12\n",
      "number of known urls:  843\n"
     ]
    }
   ],
   "source": [
    "# select the required data from Mentions Dataset\n",
    "\n",
    "# select the required data from Mentions Dataset\n",
    "mentions_1 = mentions_df.select(\"GLOBALEVENTID\", \"EventTimeDate\", 'MentionIdentifier', \"MentionType\", \"MentionSourceName\") \\\n",
    "                .filter(mentions_df[\"MentionType\"] == '1')\n",
    "\n",
    "# join the domain df with the country corresponding to the source\n",
    "mentions_2 = mentions_1.join(Domains, Domains['url'] == mentions_1['MentionSourceName'], \"left_outer\")\n",
    "\n",
    "# filter out urls that are unknown\n",
    "mentions_3 = mentions_2.filter(mentions_2.country_source.isNotNull())\n",
    "\n",
    "# print the number of urls that have no country\n",
    "print('number of unknown urls: ', mentions_2.filter(\"url is null\").select('MentionSourceName').distinct().count())\n",
    "\n",
    "# print the number of urls associated to a country\n",
    "print('number of known urls: ', mentions_3.select('url').distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only a small fraction of 'news urls' are unknown in terms of which country they belong to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the file with the countries and capitals\n",
    "mentions_4 = mentions_3.join(CountryToCapital, CountryToCapital['CountryName'] == mentions_3['country_source'], \"left_outer\") \n",
    "\n",
    "# filter out rows with no geographic coordinates\n",
    "mentions_5 = mentions_4.filter(mentions_4.CapitalLatitude.isNotNull())\n",
    "\n",
    "# select relevant columns \n",
    "mentions_6 = mentions_5.select('GLOBALEVENTID', 'MentionIdentifier','EventTimeDate','CountryCode', 'CountryName', mentions_5['CapitalLatitude'].alias('Source_Lat'),\n",
    "                                                   mentions_5['CapitalLongitude'].alias('Source_Long'))\n",
    "mentions_6 = mentions_6.withColumn(\"Source_Lat\", mentions_6[\"Source_Lat\"].cast(\"float\"))\n",
    "mentions_6 = mentions_6.withColumn(\"Source_Long\", mentions_6[\"Source_Long\"].cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare gkg file\n",
    "gkg_1 = gkg_df.select('DocumentIdentifier', 'V2Tone')\n",
    "split_col = split(gkg_df['V2Tone'], ',')\n",
    "gkg_2 = gkg_1.withColumn('Tone', split_col.getItem(0))\n",
    "gkg_3 = gkg_2.withColumn('Polarity', split_col.getItem(3))\n",
    "gkg_4 = gkg_3.withColumn('Emotion_Charge', abs(gkg_3.Tone - gkg_3.Polarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of events:  5251\n",
      "Number of events with geographic coordinates:  5095\n",
      "+-------------+---------------------+-------------+--------------+-----------+----------+-----------+----------+--------------------+--------------+-----------+-----------+----------+-----------+--------------------+----------------+\n",
      "|GLOBALEVENTID|ActionGeo_CountryCode|ActionGeo_Lat|ActionGeo_Long|NumMentions|NumSources|NumArticles|   AvgTone|   MentionIdentifier| EventTimeDate|CountryCode|CountryName|Source_Lat|Source_Long|  DocumentIdentifier|  Emotion_Charge|\n",
      "+-------------+---------------------+-------------+--------------+-----------+----------+-----------+----------+--------------------+--------------+-----------+-----------+----------+-----------+--------------------+----------------+\n",
      "|    709307290|                   AF|      34.5167|       69.1833|          6|         1|          6|-3.5294118|http://www.khaama...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|http://www.khaama...|9.81595092024539|\n",
      "|    709307290|                   AF|      34.5167|       69.1833|          6|         1|          6|-3.5294118|http://www.khaama...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|http://www.khaama...|9.81595092024539|\n",
      "|    709307289|                   AF|      34.5167|       69.1833|          4|         1|          4|-3.5294118|http://www.khaama...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|http://www.khaama...|9.81595092024539|\n",
      "|    709307289|                   AF|      34.5167|       69.1833|          4|         1|          4|-3.5294118|http://www.khaama...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|http://www.khaama...|9.81595092024539|\n",
      "|    709306810|                   IN|      30.6456|       76.3825|          6|         1|          6| 2.4590163|https://www.pajhw...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|https://www.pajhw...|0.82304526748971|\n",
      "+-------------+---------------------+-------------+--------------+-----------+----------+-----------+----------+--------------------+--------------+-----------+-----------+----------+-----------+--------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select Data from Events Dataset\n",
    "events_1= events_df.select(\"GLOBALEVENTID\", 'ActionGeo_CountryCode', \"ActionGeo_Lat\", \"ActionGeo_Long\", \"NumMentions\",\"NumSources\",\"NumArticles\",\"AvgTone\")\n",
    "\n",
    "# filter out events that have no geographic coordinates\n",
    "print('Total number of events: ', events_1.count())\n",
    "events_2 = events_1.filter(events_1.ActionGeo_Lat.isNotNull())\n",
    "print('Number of events with geographic coordinates: ', events_2.count())\n",
    "\n",
    "# merge the clean events and mentions dfs\n",
    "event_mention = events_2.join(mentions_6, 'GLOBALEVENTID') \n",
    "\n",
    "data_q1 = event_mention.join(gkg_4.select('DocumentIdentifier', 'Emotion_Charge'), gkg_4['DocumentIdentifier'] == event_mention['MentionIdentifier'])\n",
    "data_q1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the events reported by GDELT have a reported geographic location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the distance between the source and the event\n",
    "# append a column with the geographic distance\n",
    "def geocalc(lat1, lon1, lat2, lon2):\n",
    "    #print(lat1, lon1, lat2, lon2)\n",
    "    lat1 = radians(lat1)\n",
    "    lon1 = radians(lon1)\n",
    "    lat2 = radians(lat2)\n",
    "    lon2 = radians(lon2)\n",
    "    \n",
    "    dlon = lon1 - lon2\n",
    "\n",
    "    EARTH_R = 6372.8\n",
    "\n",
    "    y = sqrt((cos(lat2) * sin(dlon)) ** 2 + (cos(lat1) * sin(lat2) - sin(lat1) * cos(lat2) * cos(dlon)) ** 2)\n",
    "    x = sin(lat1) * sin(lat2) + cos(lat1) * cos(lat2) * cos(dlon)\n",
    "    c = atan2(y, x)\n",
    "    return EARTH_R * c\n",
    "\n",
    "udf_geocalc = udf(geocalc, FloatType())\n",
    " \n",
    "data_q1_1 = data_q1.withColumn(\"distance\", lit(udf_geocalc('ActionGeo_Lat', 'ActionGeo_Long', 'Source_Lat', 'Source_Long')))\n",
    "\n",
    "data_q1_2 = data_q1_1.select('ActionGeo_CountryCode', 'distance', 'AvgTone','NumArticles', data_q1_1['CountryName'].alias('SourceCountry'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applying the algorithm on the whole GDELT dataset, a dataset of 3.4 GB was obtained. Since the goal was to plot each event as a datapoint in a graph, this dataset was randomly downsampled and only 0.01 % were kept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Are some countries ignored in the news?<a id='Part2_Q2'></a>\n",
    "\n",
    "The whole fetching cluster code is found in the [Q2_cluster.py](FetchingData/Q2_cluster.py) file, which is very similar to the cluster 1 code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------------+------------+\n",
      "|ActionGeo_CountryCode|sum_articles|count_events|\n",
      "+---------------------+------------+------------+\n",
      "|                   CI|         111|          15|\n",
      "|                   FI|           4|           1|\n",
      "|                   IC|           2|           1|\n",
      "|                   RO|          23|           9|\n",
      "|                   SL|          31|           7|\n",
      "+---------------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_q2 = events_2.groupBy('ActionGeo_CountryCode').agg(sum('NumArticles').alias('sum_articles'), count('GLOBALEVENTID').alias('count_events'))\n",
    "data_q2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Are we emotionally predictable? <a id='Part2_Q3a'></a>\n",
    "\n",
    "### Fetching the data\n",
    "\n",
    "The whole cluster code is found in the [Q3_cluster.py](FetchingData/Q3_cluster.py) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+------------+----------+-------+\n",
      "|_c0|    Country|      HDI|Country name|FIPS_GDELT|alpha-2|\n",
      "+---+-----------+---------+------------+----------+-------+\n",
      "|  0|     Norway|Very High|      Norway|        NO|     NO|\n",
      "|  1|Switzerland|Very High| Switzerland|        SZ|     CH|\n",
      "|  2|  Australia|Very High|   Australia|        AS|     AU|\n",
      "|  3|    Ireland|Very High|     Ireland|        EI|     IE|\n",
      "|  4|    Germany|Very High|     Germany|        GM|     DE|\n",
      "+---+-----------+---------+------------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# open helper dataset\n",
    "HDI_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../data/CountryInformations/HDI_code_df.csv\")\n",
    "HDI_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The human development index is divided into 4 categories: Very High, High, Medium, Low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter on events that have count information\n",
    "gkg_1 = gkg_df.filter(gkg_df.Counts.isNotNull())\n",
    "CountType = split(gkg_1['Counts'], '#')\n",
    "\n",
    "# add the event type\n",
    "gkg_2 = gkg_1.withColumn('EventType', CountType.getItem(0))\n",
    "\n",
    "# add the number of people concerned\n",
    "gkg_3 = gkg_2.withColumn('NumPeople', CountType.getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare mention file\n",
    "mentions_1 = mentions_df.select(\"GLOBALEVENTID\", \"MentionType\", \"MentionSourceName\", 'MentionIdentifier') \\\n",
    "                .filter(mentions_df[\"MentionType\"] == '1')\n",
    "\n",
    "# join the dataframe url to country\n",
    "mentions_2 = mentions_1.join(Domains, Domains['url'] == mentions_1['MentionSourceName'], \"left_outer\") \n",
    "\n",
    "# filter out urls that are unknown\n",
    "mentions_3 = mentions_2.filter(mentions_2.country_source.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_1= events_df.select(\"GLOBALEVENTID\", 'Day_DATE','Actor1Religion1Code', 'Actor2Religion1Code',\n",
    "                               'NumMentions',\"AvgTone\", 'GoldsteinScale', 'ActionGeo_CountryCode')\n",
    "\n",
    "# filter events with no country code\n",
    "events_2 = events_1.filter(events_1.ActionGeo_CountryCode.isNotNull())\n",
    "\n",
    "# create religion column (take one of the two actors, because in the example test there was never data for both at the same time)\n",
    "events_3 = events_2.withColumn('ActorReligion', coalesce(events_2['Actor1Religion1Code'], events_2['Actor2Religion1Code']))\n",
    "\n",
    "# filter out columns with no religion\n",
    "events_4 = events_3.filter(events_3.ActorReligion.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### join the files togther\n",
    "\n",
    "# join the HDI file to country code\n",
    "events_5 = events_4.join(HDI_df.select('Country', 'HDI', 'FIPS_GDELT'), HDI_df['FIPS_GDELT']==events_4['ActionGeo_CountryCode'])\n",
    "\n",
    "# join event and mention file\n",
    "mention_event = events_5.join(mentions_3, 'GLOBALEVENTID')\n",
    "\n",
    "# join mention_event and gkg\n",
    "gkg_mention_event = mention_event.join(gkg_3, mention_event['MentionIdentifier'] == gkg_3['DocumentIdentifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gkg, original size:  6098\n",
      "gkg, filtered:  772\n",
      "mention, original size:  13508\n",
      "mention, filtered:  13421\n",
      "event, original size:  5251\n",
      "event, filtered:  199\n",
      "files joined together:  200\n",
      "all:  49\n"
     ]
    }
   ],
   "source": [
    "print('gkg, original size: ', gkg_df.count())\n",
    "print('gkg, filtered: ', gkg_3.count())\n",
    "print('mention, original size: ', mentions_df.count())\n",
    "print('mention, filtered: ', mentions_3.count())\n",
    "print('event, original size: ', events_df.count())\n",
    "print('event, filtered: ', events_4.count())\n",
    "print('files joined together: ', mention_event.count())\n",
    "print('all: ', gkg_mention_event.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the filtering is reduces the size of the datafiles quite drastically. This is because in the gkg file, most of the articles do not have a specified event type, in the event file, very few events have the religion of the actors defined, and in the mention file, not every article can be linked to a country source reporting the article.\n",
    "\n",
    "In the following we will select the emotions of interest from the GCAM data column available in the gkg file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "HRE = ['c2.152', 'c2.214', 'c3.2', 'c5.7', 'c6.5', 'c7.2', 'c10.1',\n",
    "       'c14.9', 'c15.3', 'c15.4', 'c15.12', 'c15.26', 'c15.27', 'c15.30',\n",
    "       'c15.36', 'c15.42', 'c15.53', 'c15.57', 'c15.61', 'c15.92',\n",
    "       'c15.93', 'c15.94', 'c15.97', 'c15.101', 'c15.102', 'c15.103',\n",
    "       'c15.105', 'c15.106', 'c15.107', 'c15.108', 'c15.109', 'c15.110',\n",
    "       'c15.116', 'c15.120', 'c15.123', 'c15.126', 'c15.131', 'c15.136',\n",
    "       'c15.137', 'c15.152', 'c15.171', 'c15.173', 'c15.179', 'c15.203',\n",
    "       'c15.219', 'c15.221', 'c15.239', 'c15.260', 'c21.1', 'c35.31',\n",
    "       'c24.1', 'c24.2', 'c24.3', 'c24.4', 'c24.5', 'c24.6', 'c24.7',\n",
    "       'c24.8', 'c24.9', 'c24.10', 'c24.11', 'c36.31', 'c37.31', 'c41.1']\n",
    "\n",
    "\n",
    "Emot_Words_df = gkg_mention_event.select(gkg_mention_event['ActionGeo_CountryCode'].alias('CountryEvent'), 'EventType',\n",
    "                                   'ActorReligion', 'HDI', 'AvgTone',\n",
    "                                  'country_source', 'GCAM',split(col(\"GCAM\"), \":\").alias(\"GCAM2\"))\n",
    "\n",
    "Emot_Words_df = Emot_Words_df.withColumn('GCAM2', concat_ws(',', 'GCAM2'))\n",
    "\n",
    "Emot_Words_df = Emot_Words_df.select('CountryEvent', 'EventType','ActorReligion', 'country_source',\n",
    "                                     'HDI', 'AvgTone', split(col(\"GCAM2\"), \",\").alias(\"GCAM\"))\n",
    "\n",
    "Emot_Words_df = Emot_Words_df.withColumn(\"HRE\", array([lit(x) for x in HRE]) )\n",
    "\n",
    "differencer = udf(lambda x,y: list(set(y)-(set(y)-set(x))), ArrayType(StringType()))\n",
    "Emot_Words_df = Emot_Words_df.withColumn('DIF', differencer('HRE', 'GCAM'))\n",
    "\n",
    "Emot_Words_df = Emot_Words_df.select('CountryEvent', 'EventType','ActorReligion', 'country_source',\n",
    "                                     'HDI', 'AvgTone', 'DIF')\n",
    "\n",
    "data_q3 = Emot_Words_df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------------+--------------------+------+----------+--------------------+\n",
      "|CountryEvent|EventType|ActorReligion|      country_source|   HDI|   AvgTone|                 DIF|\n",
      "+------------+---------+-------------+--------------------+------+----------+--------------------+\n",
      "|          BK|     KILL|          MOS|United States of ...|  High|-10.911809|[c15.110, c15.103...|\n",
      "|          MR|  PROTEST|          MOS|              Turkey|   Low|-8.5561495|[c2.214, c41.1, c...|\n",
      "|          ID|   AFFECT|          MOS|            Malaysia|Medium|-2.7181687|[c2.214, c14.9, c...|\n",
      "|          PK|  PROTEST|          MOS|         Philippines|Medium|-3.1468532|[c15.110, c14.9, ...|\n",
      "|          IZ|  PROTEST|          MOS|        Saudi Arabia|Medium| 1.9607843|[c15.171, c2.214,...|\n",
      "+------------+---------+-------------+--------------------+------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_q3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_q3_strat = data_q3.sampleBy('EventType', fractions={'AFFECT': 0.1, 'ARMEDCONFLICT': 1.0, 'ARREST': 0.1, 'ASSASSINATION': 1,\n",
    "                                                                'EVACUATION': 1, 'HUMAN_RIGHTS_ABUSES_FORCED_MIGRATION': 1, 'KIDNAP': 0.1,\n",
    "                                                                'KILL': 0.01, 'MOVEMENT_GENERAL': 1, 'POVERTY': 1, 'PROTEST': 0.1,\n",
    "                                                                'REFUGEES': 1, 'RELEASE_HOSTAGE': 1, 'STRIKE': 1, 'TERROR': 1, 'WOUND': 0.1,\n",
    "                                                                'SEIZE': 0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset serves to build a model in the following. Due to the drastic filtering processing, the algorithm applied on the whole GDELT dataset returned a dataset which we only needed to downsample a little bit, which we did by stratifying the EventType column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model <a id='Part2_Q3b'></a>\n",
    "\n",
    "This part we do on the downsampled data we gathered from the cluster. We trained the model with the scikit decision tree algorithm and once with the random forest algorithm. The test accuracy was very similar, however the decision tree overfit the training data too much and thus we decided to use the random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ActorReligion</th>\n",
       "      <th>AvgTone</th>\n",
       "      <th>CountryEvent</th>\n",
       "      <th>DIF</th>\n",
       "      <th>Day_DATE</th>\n",
       "      <th>Emotion_Charge</th>\n",
       "      <th>EventType</th>\n",
       "      <th>HDI</th>\n",
       "      <th>NumPeople</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Tone</th>\n",
       "      <th>country_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MOS</td>\n",
       "      <td>-8.812949</td>\n",
       "      <td>PK</td>\n",
       "      <td>[c15.260, c15.93, c2.214, c15.107, c6.5, c7.2,...</td>\n",
       "      <td>20161116</td>\n",
       "      <td>17.173524</td>\n",
       "      <td>WOUND</td>\n",
       "      <td>Medium</td>\n",
       "      <td>3</td>\n",
       "      <td>8.586762</td>\n",
       "      <td>-8.586762</td>\n",
       "      <td>Pakistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MOS</td>\n",
       "      <td>-6.274140</td>\n",
       "      <td>SY</td>\n",
       "      <td>[c15.123, c2.214, c15.97, c6.5, c15.110, c5.7,...</td>\n",
       "      <td>20170411</td>\n",
       "      <td>13.211845</td>\n",
       "      <td>ARREST</td>\n",
       "      <td>Low</td>\n",
       "      <td>8</td>\n",
       "      <td>7.744875</td>\n",
       "      <td>-5.466970</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ActorReligion   AvgTone CountryEvent  \\\n",
       "0           MOS -8.812949           PK   \n",
       "1           MOS -6.274140           SY   \n",
       "\n",
       "                                                 DIF  Day_DATE  \\\n",
       "0  [c15.260, c15.93, c2.214, c15.107, c6.5, c7.2,...  20161116   \n",
       "1  [c15.123, c2.214, c15.97, c6.5, c15.110, c5.7,...  20170411   \n",
       "\n",
       "   Emotion_Charge EventType     HDI  NumPeople  Polarity      Tone  \\\n",
       "0       17.173524     WOUND  Medium          3  8.586762 -8.586762   \n",
       "1       13.211845    ARREST     Low          8  7.744875 -5.466970   \n",
       "\n",
       "  country_source  \n",
       "0       Pakistan  \n",
       "1         Canada  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data = pd.read_json('../data/data_q3_tone.json', lines = True) #the data is not appended in Github due to the large file size\n",
    "model_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open helper datasets\n",
    "country_info = pd.read_csv('../data/CountryCodes/countries-info.csv')\n",
    "country_fips_alpha = pd.read_csv('../data/CountryCodes/CountryCode_FIPS_alpha.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the alpha-3 country codes to the source countries\n",
    "model_data_1 = model_data.merge(country_info[['name', 'alpha-3']], left_on = 'country_source', right_on = 'name')\n",
    "model_data_1 = model_data_1.rename(columns = {'country_source': 'CountrySource', 'alpha-3': 'SourceCode'})\n",
    "model_data_1 = model_data_1.drop(columns = ['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the official alpha country codes to the GDELT country code\n",
    "model_data_2 = model_data_1.merge(country_fips_alpha, left_on = 'CountryEvent', right_on = 'FIPS_GDELT')\n",
    "model_data_2 = model_data_2.rename(columns = {'CountryEvent':'CountryEventCode', 'Country name': 'CountryEvent'})\n",
    "model_data_2 = model_data_2.drop(columns = ['FIPS_GDELT'])\n",
    "#model_data_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the alpha-3 country codes to the event countries\n",
    "model_data_3 = model_data_2.merge(country_info[['alpha-2', 'alpha-3']], left_on = 'alpha-2', right_on = 'alpha-2')\n",
    "model_data_3 = model_data_3.drop(columns = ['CountryEventCode', 'alpha-2'])\n",
    "model_data_3 = model_data_3.rename(columns = {'country_source': 'CountrySource', 'alpha-3': 'CountryEventCode'})\n",
    "#model_data_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the features into categorical data to be used by the learning algorithm\n",
    "def discretize(df):\n",
    "    model_data = df.copy()\n",
    "    # discretize the tone\n",
    "    bins = pd.IntervalIndex.from_tuples([(-101, -10), (-10, -5), (-5, 0), (0,5),\n",
    "                                         (5,101)])\n",
    "    model_data.Tone = pd.cut(model_data.Tone, bins)\n",
    "\n",
    "    # transform into categorical data & create corresponding dictionary\n",
    "    le_CountryEvent = LabelEncoder()\n",
    "    model_data['CountryEventCode'] = le_CountryEvent.fit_transform(model_data['CountryEventCode'])\n",
    "    le_CountryEvent_mapping = dict(zip(le_CountryEvent.classes_, le_CountryEvent.transform(le_CountryEvent.classes_)))\n",
    "\n",
    "    le_CountrySource = LabelEncoder()\n",
    "    model_data['SourceCode'] = le_CountrySource.fit_transform(model_data['SourceCode'])\n",
    "    le_CountrySource_mapping = dict(zip(le_CountrySource.classes_, le_CountrySource.transform(le_CountrySource.classes_)))\n",
    "\n",
    "\n",
    "    # transform back with: list(le_CountryEvent.inverse_transform([2, 2, 1]))\n",
    "    le_EventType = LabelEncoder()\n",
    "    model_data['EventType'] = le_EventType.fit_transform(model_data['EventType'])\n",
    "    le_EventType_mapping = dict(zip(le_EventType.classes_, le_EventType.transform(le_EventType.classes_)))\n",
    "\n",
    "    le_ActorReligion = LabelEncoder()\n",
    "    model_data['ActorReligion'] = le_ActorReligion.fit_transform(model_data['ActorReligion'])\n",
    "    le_ActorReligion_mapping = dict(zip(le_ActorReligion.classes_, le_ActorReligion.transform(le_ActorReligion.classes_)))\n",
    "\n",
    "    le_HDI = LabelEncoder()\n",
    "    model_data['HDI'] = le_HDI.fit_transform(model_data['HDI'])\n",
    "    le_HDI_mapping = dict(zip(le_HDI.classes_, le_HDI.transform(le_HDI.classes_)))\n",
    "\n",
    "    le_Tone = LabelEncoder()\n",
    "    model_data['Tone'] = le_Tone.fit_transform(model_data['Tone'])\n",
    "    le_Tone_mapping = dict(zip(le_Tone.classes_, le_Tone.transform(le_Tone.classes_)))\n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ActorReligion</th>\n",
       "      <th>AvgTone</th>\n",
       "      <th>DIF</th>\n",
       "      <th>Day_DATE</th>\n",
       "      <th>Emotion_Charge</th>\n",
       "      <th>EventType</th>\n",
       "      <th>HDI</th>\n",
       "      <th>NumPeople</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Tone</th>\n",
       "      <th>CountrySource</th>\n",
       "      <th>SourceCode</th>\n",
       "      <th>CountryEvent</th>\n",
       "      <th>CountryEventCode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>-8.812949</td>\n",
       "      <td>[c15.260, c15.93, c2.214, c15.107, c6.5, c7.2,...</td>\n",
       "      <td>20161116</td>\n",
       "      <td>17.173524</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>8.586762</td>\n",
       "      <td>1</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>101</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>-5.735430</td>\n",
       "      <td>[c15.92, c2.214, c6.5, c15.42, c7.2, c15.57, c...</td>\n",
       "      <td>20170323</td>\n",
       "      <td>13.381555</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>7.866184</td>\n",
       "      <td>1</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>101</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ActorReligion   AvgTone                                                DIF  \\\n",
       "0              9 -8.812949  [c15.260, c15.93, c2.214, c15.107, c6.5, c7.2,...   \n",
       "1              9 -5.735430  [c15.92, c2.214, c6.5, c15.42, c7.2, c15.57, c...   \n",
       "\n",
       "   Day_DATE  Emotion_Charge  EventType  HDI  NumPeople  Polarity  Tone  \\\n",
       "0  20161116       17.173524         16    2          3  8.586762     1   \n",
       "1  20170323       13.381555          6    2        200  7.866184     1   \n",
       "\n",
       "  CountrySource  SourceCode CountryEvent  CountryEventCode  \n",
       "0      Pakistan         101     Pakistan               123  \n",
       "1      Pakistan         101     Pakistan               123  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data = discretize(model_data_3)\n",
    "model_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X and Y\n",
    "X = model_data.values[:, [0,5,6,7,11,13]]\n",
    "Y = model_data.values[:,9]\n",
    "Y = Y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, importance = True):\n",
    "    # split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 42)\n",
    "    # train the model (we use the random forest, because the decision tree overfit the data)\n",
    "    #clf = tree.DecisionTreeClassifier()\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=4, random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    print(\"Test Accuracy is \", accuracy_score(y_test,y_pred_test)*100)\n",
    "    print(\"Train Accuracy is \", accuracy_score(y_train ,y_pred_train)*100)\n",
    "    if importance:\n",
    "        importances = clf.feature_importances_\n",
    "        print(importances)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy is  56.0315670800451\n",
      "Train Accuracy is  55.866341683399014\n",
      "[0.22240931 0.3796353  0.03289924 0.1575336  0.00932777 0.19819478]\n"
     ]
    }
   ],
   "source": [
    "# train the model and see the accuracy\n",
    "model(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model with the words\n",
    "s = model_data_2['DIF']\n",
    "dummy_words = pd.get_dummies(s.apply(pd.Series).stack()).sum(level=0)\n",
    "dummy_words = dummy_words.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ActorReligion</th>\n",
       "      <th>AvgTone</th>\n",
       "      <th>Emotion_Charge</th>\n",
       "      <th>EventType</th>\n",
       "      <th>HDI</th>\n",
       "      <th>NumPeople</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Tone</th>\n",
       "      <th>CountrySource</th>\n",
       "      <th>SourceCode</th>\n",
       "      <th>...</th>\n",
       "      <th>c15.94</th>\n",
       "      <th>c15.97</th>\n",
       "      <th>c2.152</th>\n",
       "      <th>c2.214</th>\n",
       "      <th>c3.2</th>\n",
       "      <th>c35.31</th>\n",
       "      <th>c41.1</th>\n",
       "      <th>c5.7</th>\n",
       "      <th>c6.5</th>\n",
       "      <th>c7.2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MOS</td>\n",
       "      <td>-8.812949</td>\n",
       "      <td>17.173524</td>\n",
       "      <td>WOUND</td>\n",
       "      <td>Medium</td>\n",
       "      <td>3</td>\n",
       "      <td>8.586762</td>\n",
       "      <td>-8.586762</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>PAK</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MOS</td>\n",
       "      <td>-5.735430</td>\n",
       "      <td>13.381555</td>\n",
       "      <td>KIDNAP</td>\n",
       "      <td>Medium</td>\n",
       "      <td>200</td>\n",
       "      <td>7.866184</td>\n",
       "      <td>-5.515371</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>PAK</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ActorReligion   AvgTone  Emotion_Charge EventType     HDI  NumPeople  \\\n",
       "0           MOS -8.812949       17.173524     WOUND  Medium          3   \n",
       "1           MOS -5.735430       13.381555    KIDNAP  Medium        200   \n",
       "\n",
       "   Polarity      Tone CountrySource SourceCode  ...  c15.94 c15.97  c2.152  \\\n",
       "0  8.586762 -8.586762      Pakistan        PAK  ...     0.0    0.0     0.0   \n",
       "1  7.866184 -5.515371      Pakistan        PAK  ...     0.0    0.0     0.0   \n",
       "\n",
       "   c2.214  c3.2  c35.31  c41.1  c5.7  c6.5  c7.2  \n",
       "0     1.0   1.0     1.0    0.0   1.0   1.0   1.0  \n",
       "1     1.0   1.0     1.0    0.0   1.0   1.0   1.0  \n",
       "\n",
       "[2 rows x 61 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data_4 = pd.concat([model_data_3, dummy_words], axis=1)\n",
    "model_data_4 = model_data_4.drop(columns = ['DIF', 'Day_DATE'])\n",
    "model_data_4.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy is  60.345734686208196\n",
      "Train Accuracy is  60.47158364918987\n"
     ]
    }
   ],
   "source": [
    "# do the same procedure\n",
    "model_data = discretize(model_data_4)\n",
    "X = np.array(np.delete(model_data.values, [1,2,6,7,8,10], axis=1), dtype=float)\n",
    "Y = Y = model_data.values[:,7]\n",
    "Y = Y.astype(int)\n",
    "X[np.isnan(X)] = 0\n",
    "model(X, Y, importance = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. **Do we have a saturation limit?**  <a id='Part2_Q4'></a>\n",
    "The whole cluster code is found in the [Q4_cluster.py](FetchingData/Q4_cluster.py) file.\n",
    "> In the cluster py file you will find the code that was used to run in the cluster to build the Emotion Charge for each country for that month. In this part we will focus on the dataframe we get form the parquet files obtained from the cluster code.\n",
    "\n",
    "Some references to question 5 might be made, as these questions are related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_df = concat_parquets_info(\"../data/Gdelt/Q4_parquets_csv_wReg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CountrySource</th>\n",
       "      <th>region</th>\n",
       "      <th>EmotionCharge</th>\n",
       "      <th>CountCountrySource</th>\n",
       "      <th>NormEvnt</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Korea (Republic of)</td>\n",
       "      <td>Asia</td>\n",
       "      <td>580.323195</td>\n",
       "      <td>8433</td>\n",
       "      <td>0.068816</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Philippines</td>\n",
       "      <td>Asia</td>\n",
       "      <td>6840.854788</td>\n",
       "      <td>57366</td>\n",
       "      <td>0.119249</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Malaysia</td>\n",
       "      <td>Asia</td>\n",
       "      <td>4078.438874</td>\n",
       "      <td>47824</td>\n",
       "      <td>0.085280</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fiji</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>269.846193</td>\n",
       "      <td>5039</td>\n",
       "      <td>0.053552</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>Asia</td>\n",
       "      <td>4847.623570</td>\n",
       "      <td>57565</td>\n",
       "      <td>0.084211</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         CountrySource   region  EmotionCharge  CountCountrySource  NormEvnt  \\\n",
       "0  Korea (Republic of)     Asia     580.323195                8433  0.068816   \n",
       "1          Philippines     Asia    6840.854788               57366  0.119249   \n",
       "2             Malaysia     Asia    4078.438874               47824  0.085280   \n",
       "3                 Fiji  Oceania     269.846193                5039  0.053552   \n",
       "4               Turkey     Asia    4847.623570               57565  0.084211   \n",
       "\n",
       "   year  \n",
       "0  2015  \n",
       "1  2015  \n",
       "2  2015  \n",
       "3  2015  \n",
       "4  2015  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always we see that the US dominates the number of events, and therefore obtains the biggest Emotion Charge, however that does not mean that it is the most emotional country (question 5 question). That is why that if you look at the NormEvnt it is not the biggest we have seen. The Philippines are more emotional (in the year 2015) than the US, the NormEvnt allows us to compare the countries removing the bias concerning number of events that the GDELT catches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CountrySource</th>\n",
       "      <th>region</th>\n",
       "      <th>EmotionCharge</th>\n",
       "      <th>CountCountrySource</th>\n",
       "      <th>NormEvnt</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>United States of America</td>\n",
       "      <td>Americas</td>\n",
       "      <td>641624.399395</td>\n",
       "      <td>14271395</td>\n",
       "      <td>0.044959</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               CountrySource    region  EmotionCharge  CountCountrySource  \\\n",
       "76  United States of America  Americas  641624.399395            14271395   \n",
       "\n",
       "    NormEvnt  year  \n",
       "76  0.044959  2017  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_df.loc[All_df['EmotionCharge'] == All_df[\"EmotionCharge\"].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We account for 105 countries in : Asia\n",
      "We account for 15 countries in : Oceania\n",
      "We account for 51 countries in : Africa\n",
      "We account for 48 countries in : Europe\n",
      "We account for 18 countries in : Americas\n"
     ]
    }
   ],
   "source": [
    "# make list of continents\n",
    "continents = []\n",
    "for continent in All_df['region']:\n",
    "    if continent not in continents: \n",
    "        continents.append(continent)\n",
    "        \n",
    "for reg in continents:\n",
    "    print('We account for', len(All_df.loc[All_df['region'] == reg]),\n",
    "          'countries in :', reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "EventCount = All_df.groupby(['region'])['CountCountrySource'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "region\n",
       "Africa       1898486\n",
       "Americas    31454085\n",
       "Asia         7319279\n",
       "Europe       3382402\n",
       "Oceania      2953846\n",
       "Name: CountCountrySource, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EventCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something good to take into account is, that even though Americas is the second region with the least number of countries it is the one that by far (a really huge by far) has the most events. This does not mean that in America happen the most events but it is where GDELT gets most of them.\n",
    "\n",
    "In the plot that you will see in the next section this is taken into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Who is more emotional and what do they say? <a id='Part2_Q5'></a>\n",
    "The whole cluster code is found in the [Q5_cluster.py](FetchingData/Q5_cluster.py) file.\n",
    "> For this questions we focused specifically on the words that were used for some countries in the months that we analize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to start playing with the GCAM words we must have the info of the parquet files in a dataframe\n",
    "All_df_Q5 = concat_parquets_info(\"../data/Gdelt/Q5_parquets_csv\", drop_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we must do some string operations in order to have the desired list\n",
    "All_df_Q5.SpeechWordsList = All_df_Q5.SpeechWordsList.str.replace(\"'\", \"\")\n",
    "All_df_Q5.SpeechWordsList = All_df_Q5.SpeechWordsList.str.replace(\"[\", \"\")\n",
    "All_df_Q5.SpeechWordsList = All_df_Q5.SpeechWordsList.str.replace(\"]\", \"\")\n",
    "All_df_Q5.SpeechWordsList = All_df_Q5.SpeechWordsList.str.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part is only for visualization purposes (adding another column with size referent to years)\n",
    "All_df_Q5[\"SizeViz\"] = np.where(All_df_Q5['year']== '2015', 10, 20)\n",
    "All_df_Q5.loc[All_df_Q5['year'] == '2016', 'SizeViz'] = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we want to have a list of words for each country and year\n",
    "Concat_lists = All_df_Q5.groupby(['country_source', 'year','SizeViz'])['SpeechWordsList'].sum()\n",
    "Q5_df = Concat_lists.to_frame()\n",
    "Q5_df = Q5_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dictionary with the code words and how many times were used\n",
    "All_dicts = build_dict(Q5_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now starts the part of associating the words with the codes, for this we will used our own built list of human readable words. You can find how we built this dictionary in [HumanReadableEmotions_creation.py](../data/Gdelt/HumanReadableEmotions_creation.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "HRE = pd.read_csv(\"../data/Gdelt/Human_Readable_Emotions_v2\",sep='\\t')\n",
    "HRE = HRE.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_dicts_Words = dict_with_words(All_dicts,Q5_df,HRE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8348,\n",
       " {'futurity': 25,\n",
       "  'priority': 14,\n",
       "  'posteriority': 20,\n",
       "  'preterition': 21,\n",
       "  'newness': 18,\n",
       "  'work': 27,\n",
       "  'time': 22,\n",
       "  'death': 8,\n",
       "  'religion': 1,\n",
       "  'leisure': 28,\n",
       "  'achievement': 28,\n",
       "  'money': 28,\n",
       "  'home': 17,\n",
       "  'negative': 28,\n",
       "  'positive': 28,\n",
       "  'air': 15,\n",
       "  'sanguine': 8,\n",
       "  'fire': 7,\n",
       "  'mythology': 17,\n",
       "  'music': 28,\n",
       "  'mechanics': 27,\n",
       "  'mathematics': 28,\n",
       "  'meteorology': 26,\n",
       "  'medicine': 28,\n",
       "  'military': 3,\n",
       "  'metrology': 28,\n",
       "  'mountaineering': 15,\n",
       "  'acad': 28,\n",
       "  'arousal': 26,\n",
       "  'begin': 22,\n",
       "  'be': 28,\n",
       "  'earliness': 23,\n",
       "  'photography': 25,\n",
       "  'philosophy': 28,\n",
       "  'physiology': 28,\n",
       "  'physics': 28,\n",
       "  'pharmacy': 20,\n",
       "  'person': 28,\n",
       "  'philology': 24,\n",
       "  'plants': 28,\n",
       "  'noun': 28,\n",
       "  'name': 24,\n",
       "  'need': 25,\n",
       "  'negate': 21,\n",
       "  'no': 8,\n",
       "  'forgiveness': 5,\n",
       "  'sure': 28,\n",
       "  'fearlessness': 9,\n",
       "  'favor': 16,\n",
       "  'indef': 28,\n",
       "  'if': 27,\n",
       "  'impers': 28,\n",
       "  'incr': 23,\n",
       "  'peace': 4,\n",
       "  'optimism': 10,\n",
       "  'captivation': 11,\n",
       "  'joylessness': 4,\n",
       "  'statistics': 14,\n",
       "  'jollity': 1,\n",
       "  'skating': 7,\n",
       "  'fashion': 28,\n",
       "  'ethnology': 19,\n",
       "  'factotum': 28,\n",
       "  'exchange': 28,\n",
       "  'engineering': 19,\n",
       "  'electrotechnology': 18,\n",
       "  'entomology': 23,\n",
       "  'enterprise': 28,\n",
       "  'surgery': 13,\n",
       "  'sport': 28,\n",
       "  'calmness': 9,\n",
       "  'buoyancy': 3,\n",
       "  'sociology': 28,\n",
       "  'cheerlessness': 4,\n",
       "  'cheerfulness': 3,\n",
       "  'sexuality': 28,\n",
       "  'carefreeness': 3,\n",
       "  'anger': 8,\n",
       "  'sad': 26,\n",
       "  'affect': 5,\n",
       "  'humans': 20,\n",
       "  'family': 12,\n",
       "  'nautical': 28,\n",
       "  'number': 27,\n",
       "  'pedagogy': 28,\n",
       "  'ends': 27,\n",
       "  'ed': 28,\n",
       "  'econ': 28,\n",
       "  'dim': 18,\n",
       "  'quan': 28,\n",
       "  'fulfillment': 6,\n",
       "  'solicitude': 4,\n",
       "  'gladness': 6,\n",
       "  'gloom': 10,\n",
       "  'architecture': 24,\n",
       "  'artisanship': 18,\n",
       "  'art': 28,\n",
       "  'astrology': 12,\n",
       "  'astronomy': 28,\n",
       "  'aviation': 28,\n",
       "  'happiness': 7,\n",
       "  'security': 7,\n",
       "  'discomfiture': 6,\n",
       "  'crime': 20,\n",
       "  'energy': 6,\n",
       "  'finance': 25,\n",
       "  'numismatics': 6,\n",
       "  'occultism': 12,\n",
       "  'present': 22,\n",
       "  'future': 27,\n",
       "  'painting': 28,\n",
       "  'housing': 9,\n",
       "  'optics': 26,\n",
       "  'quant': 28,\n",
       "  'social': 27,\n",
       "  'adverbs': 28,\n",
       "  'prep': 28,\n",
       "  'conj': 28,\n",
       "  'arenas': 26,\n",
       "  'sky': 11,\n",
       "  'space': 28,\n",
       "  'solve': 25,\n",
       "  'stay': 13,\n",
       "  'trans': 27,\n",
       "  'our': 22,\n",
       "  'object': 28,\n",
       "  'other': 28,\n",
       "  'ought': 17,\n",
       "  'place': 28,\n",
       "  'polit': 28,\n",
       "  'gastronomy': 28,\n",
       "  'genetics': 28,\n",
       "  'healthcare': 6,\n",
       "  'folklore': 28,\n",
       "  'food': 10,\n",
       "  'football': 27,\n",
       "  'furniture': 27,\n",
       "  'baseball': 27,\n",
       "  'sculpture': 28,\n",
       "  'school': 28,\n",
       "  'racing': 26,\n",
       "  'weak': 28,\n",
       "  'admiration': 13,\n",
       "  'democracy': 12,\n",
       "  'trait': 28,\n",
       "  'neverness': 27,\n",
       "  'period': 28,\n",
       "  'approval': 15,\n",
       "  'hostile': 25,\n",
       "  'gen': 23,\n",
       "  'fetch': 22,\n",
       "  'goal': 22,\n",
       "  'urban': 5,\n",
       "  's': 28,\n",
       "  'route': 9,\n",
       "  'role': 28,\n",
       "  'ritual': 26,\n",
       "  'relig': 22,\n",
       "  'rel': 28,\n",
       "  'region': 25,\n",
       "  'race': 4,\n",
       "  'certain': 25,\n",
       "  'gratefulness': 3,\n",
       "  'gratitude': 5,\n",
       "  'eagerness': 3,\n",
       "  'elation': 7,\n",
       "  'tentative': 28,\n",
       "  'occasion': 10,\n",
       "  'past': 28,\n",
       "  'bowling': 18,\n",
       "  'betting': 22,\n",
       "  'biology': 27,\n",
       "  'badminton': 9,\n",
       "  'banking': 25,\n",
       "  'macroeconomics': 6,\n",
       "  'uncertainty': 19,\n",
       "  'litigious': 27,\n",
       "  'satisfaction': 5,\n",
       "  'reverence': 6,\n",
       "  'sadness': 10,\n",
       "  'affiliation': 27,\n",
       "  'power': 28,\n",
       "  'university': 26,\n",
       "  'vehicles': 23,\n",
       "  'coolness': 5,\n",
       "  'tourism': 11,\n",
       "  'transport': 28,\n",
       "  'election': 12,\n",
       "  'wrestling': 4,\n",
       "  'def': 28,\n",
       "  'comp': 24,\n",
       "  'they': 15,\n",
       "  'verb': 28,\n",
       "  'vary': 25,\n",
       "  'verbs': 28,\n",
       "  'article': 28,\n",
       "  'we': 22,\n",
       "  'i': 20,\n",
       "  'law': 28,\n",
       "  'justice': 3,\n",
       "  'self': 20,\n",
       "  'pron': 28,\n",
       "  'tranquillity': 4,\n",
       "  'hopefulness': 20,\n",
       "  'heraldry': 19,\n",
       "  'health': 23,\n",
       "  'grammar': 26,\n",
       "  'golf': 25,\n",
       "  'geometry': 27,\n",
       "  'geology': 27,\n",
       "  'geography': 28,\n",
       "  'history': 28,\n",
       "  'fishing': 17,\n",
       "  'quality': 28,\n",
       "  'medical': 4,\n",
       "  'play': 28,\n",
       "  'politics': 28,\n",
       "  'post': 25,\n",
       "  'psychoanalysis': 25,\n",
       "  'psychology': 28,\n",
       "  'publishing': 28,\n",
       "  'feel': 23,\n",
       "  'hear': 22,\n",
       "  'motion': 26,\n",
       "  'relativeness': 28,\n",
       "  'body': 17,\n",
       "  'linguistics': 28,\n",
       "  'literature': 28,\n",
       "  'hunting': 19,\n",
       "  'hydraulics': 20,\n",
       "  'jewellery': 11,\n",
       "  'culture': 28,\n",
       "  'industry': 28,\n",
       "  'insurance': 21,\n",
       "  'leader': 20,\n",
       "  'form': 28,\n",
       "  'freq': 25,\n",
       "  'fail': 21,\n",
       "  'fall': 10,\n",
       "  'exch': 18,\n",
       "  'exert': 25,\n",
       "  'amicability': 5,\n",
       "  'female': 5,\n",
       "  'pronoun': 28,\n",
       "  'not': 21,\n",
       "  'numb': 21,\n",
       "  'milit': 20,\n",
       "  'modif': 28,\n",
       "  'envy': 16,\n",
       "  'distress': 9,\n",
       "  'enthusiasm': 6,\n",
       "  'encouragement': 12,\n",
       "  'male': 23,\n",
       "  'kin': 11,\n",
       "  'know': 28,\n",
       "  'ly': 28,\n",
       "  'legal': 27,\n",
       "  'land': 17,\n",
       "  'vice': 26,\n",
       "  'morning': 8,\n",
       "  'chemistry': 28,\n",
       "  'card': 1,\n",
       "  'buildings': 28,\n",
       "  'boxing': 16,\n",
       "  'commerce': 28,\n",
       "  'color': 2,\n",
       "  'cinema': 28,\n",
       "  'chess': 20,\n",
       "  'virtue': 28,\n",
       "  'archaeology': 16,\n",
       "  'acoustics': 21,\n",
       "  'agriculture': 28,\n",
       "  'administration': 28,\n",
       "  'anatomy': 28,\n",
       "  'anthropology': 27,\n",
       "  'animals': 28,\n",
       "  'electricity': 26,\n",
       "  'electronics': 21,\n",
       "  'earth': 3,\n",
       "  'economy': 28,\n",
       "  'drawing': 28,\n",
       "  'diplomacy': 22,\n",
       "  'dance': 28,\n",
       "  'telecommunication': 28,\n",
       "  'tax': 18,\n",
       "  'telephony': 23,\n",
       "  'theatre': 27,\n",
       "  'tennis': 21,\n",
       "  'theology': 16,\n",
       "  'confidence': 6,\n",
       "  'causal': 27,\n",
       "  'coll': 28,\n",
       "  'perception': 24,\n",
       "  'shame': 8,\n",
       "  'inclusion': 27,\n",
       "  'exclusion': 26,\n",
       "  'tool': 27,\n",
       "  'inhibition': 27,\n",
       "  'try': 24,\n",
       "  'travel': 26,\n",
       "  'insight': 26,\n",
       "  'cause': 27,\n",
       "  'sensation': 26,\n",
       "  'behaviour': 28,\n",
       "  'attitude': 28,\n",
       "  'mood': 15,\n",
       "  'emotion': 28,\n",
       "  'protest': 5,\n",
       "  'kill': 8,\n",
       "  'eval': 26,\n",
       "  'er': 24,\n",
       "  'est': 21,\n",
       "  'pleasure': 25,\n",
       "  'qual': 25,\n",
       "  'hostility': 4,\n",
       "  'surprise': 4,\n",
       "  'stir': 17,\n",
       "  'assent': 2,\n",
       "  'humanities': 23,\n",
       "  'easiness': 4,\n",
       "  'frequency': 5,\n",
       "  'regularity': 4,\n",
       "  'basketball': 17,\n",
       "  'oldness': 18,\n",
       "  'delay': 3,\n",
       "  'age': 12,\n",
       "  'placidity': 4,\n",
       "  'swimming': 14,\n",
       "  'comfortableness': 4,\n",
       "  'volleyball': 5,\n",
       "  'persist': 24,\n",
       "  'hockey': 12,\n",
       "  'yes': 20,\n",
       "  'dist': 3,\n",
       "  'astronautics': 7,\n",
       "  'finish': 20,\n",
       "  'environment': 7,\n",
       "  'corruption': 11,\n",
       "  'smuggling': 1,\n",
       "  'seize': 11,\n",
       "  'rise': 15,\n",
       "  'transientness': 9,\n",
       "  'sanctions': 1,\n",
       "  'defence': 1,\n",
       "  'legislation': 6,\n",
       "  'anxiety': 10,\n",
       "  'vehicle': 6,\n",
       "  'depression': 11,\n",
       "  'devotion': 4,\n",
       "  'lateness': 7,\n",
       "  'cricket': 8,\n",
       "  'aesthetic': 8,\n",
       "  'annoyance': 1,\n",
       "  'anxiousness': 1,\n",
       "  'dentistry': 9,\n",
       "  'levity': 3,\n",
       "  'see': 11,\n",
       "  'fencing': 12,\n",
       "  'soccer': 14,\n",
       "  'pain': 14,\n",
       "  'friends': 4,\n",
       "  'hopelessness': 14,\n",
       "  'helplessness': 5,\n",
       "  'railway': 6,\n",
       "  'alliance': 5,\n",
       "  'arrest': 7,\n",
       "  'affection': 11,\n",
       "  'astonishment': 5,\n",
       "  'water': 7,\n",
       "  'aud': 14,\n",
       "  'aquatic': 9,\n",
       "  'fondness': 11,\n",
       "  'regard': 6,\n",
       "  'ingest': 10,\n",
       "  'daze': 2,\n",
       "  'protectiveness': 5,\n",
       "  'education': 10,\n",
       "  'you': 14,\n",
       "  'compassion': 7,\n",
       "  'negotiations': 4,\n",
       "  'love': 16,\n",
       "  'tenderness': 6,\n",
       "  'stupefaction': 3,\n",
       "  'softheartedness': 6,\n",
       "  'attachment': 4,\n",
       "  'indifference': 5,\n",
       "  'melancholy': 1,\n",
       "  'despair': 8,\n",
       "  'downheartedness': 7,\n",
       "  'despondency': 6,\n",
       "  'discouragement': 11,\n",
       "  'resignation': 5,\n",
       "  'forlornness': 7,\n",
       "  'paranormal': 7,\n",
       "  'pessimism': 8,\n",
       "  'extremism': 3,\n",
       "  'friendliness': 7,\n",
       "  'sexual': 5,\n",
       "  'togetherness': 14,\n",
       "  'emotionlessness': 3,\n",
       "  'evening': 7,\n",
       "  'oceanography': 3,\n",
       "  'closeness': 14,\n",
       "  'liking': 13,\n",
       "  'preference': 7,\n",
       "  'dislike': 7,\n",
       "  'rowing': 10,\n",
       "  'belonging': 11,\n",
       "  'maritime': 3,\n",
       "  'skiing': 8,\n",
       "  'frustration': 6,\n",
       "  'defeatism': 9,\n",
       "  'athletics': 10,\n",
       "  'unemployment': 5,\n",
       "  'ani': 6,\n",
       "  'topography': 5,\n",
       "  'disapproval': 3,\n",
       "  'radiology': 1,\n",
       "  'pensiveness': 1,\n",
       "  'joy': 2,\n",
       "  'dysphoria': 1,\n",
       "  'weepiness': 1,\n",
       "  'woe': 1,\n",
       "  'attrition': 2,\n",
       "  'apathy': 2,\n",
       "  'gravity': 4,\n",
       "  'misery': 2,\n",
       "  'rejoicing': 2,\n",
       "  'repentance': 2,\n",
       "  'heavyheartedness': 1,\n",
       "  'link': 4,\n",
       "  'mournfulness': 1,\n",
       "  'telegraphy': 1,\n",
       "  'humility': 3,\n",
       "  'sympathy': 4,\n",
       "  'thing': 3,\n",
       "  'ingratitude': 2,\n",
       "  'dolefulness': 1,\n",
       "  'evacuation': 1,\n",
       "  'benevolence': 2,\n",
       "  'beneficence': 1,\n",
       "  'diffidence': 1,\n",
       "  'confusion': 3,\n",
       "  'anticipation': 8,\n",
       "  'embarrassment': 3,\n",
       "  'fitfulness': 1,\n",
       "  'biochemistry': 1,\n",
       "  'malevolence': 1,\n",
       "  'malice': 1,\n",
       "  'tumult': 1,\n",
       "  'trial': 6,\n",
       "  'distance': 5,\n",
       "  'youth': 1,\n",
       "  'nausea': 1,\n",
       "  'think': 1,\n",
       "  'identification': 4,\n",
       "  'fury': 1,\n",
       "  'repugnance': 4,\n",
       "  'wrath': 1,\n",
       "  'lividity': 1,\n",
       "  'chagrin': 1,\n",
       "  'transportation': 6,\n",
       "  'disinclination': 3,\n",
       "  'psychiatry': 5,\n",
       "  'poverty': 1,\n",
       "  'anomie': 6,\n",
       "  'weakness': 1,\n",
       "  'warmheartedness': 2,\n",
       "  'contentment': 2,\n",
       "  'infrequency': 1,\n",
       "  'disgust': 3,\n",
       "  'displeasure': 2,\n",
       "  'traffic': 1,\n",
       "  'oppression': 1,\n",
       "  'weight': 1,\n",
       "  'lovingness': 2,\n",
       "  'ban': 2,\n",
       "  'appointment': 2,\n",
       "  'earnestness': 2,\n",
       "  'antipathy': 1,\n",
       "  'loyalty': 1,\n",
       "  'panic': 1,\n",
       "  'isolation': 1,\n",
       "  'insecurity': 1,\n",
       "  'chill': 1,\n",
       "  'persecution': 1,\n",
       "  'hate': 1,\n",
       "  'scare': 1,\n",
       "  'discrimination': 1,\n",
       "  'worship': 1,\n",
       "  'ideology': 1,\n",
       "  'grief': 1,\n",
       "  'labour': 1,\n",
       "  'horror': 1,\n",
       "  'amorousness': 1,\n",
       "  'forestry': 2,\n",
       "  'fit': 2,\n",
       "  'unfriendliness': 1,\n",
       "  'brotherhood': 1,\n",
       "  'relations': 1,\n",
       "  'chronometry': 2})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of random country and words associated\n",
    "All_dicts_Words[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the list you see the words used for every country during the years 2015, 2016 and 2017. This list was made in ordered to have a subset of the GCAM feature but with human readable words. As you saw from our model, adding 64 chosen words (mostly positive) we obtained a gain in terms of accuracy, something that sustains our principle that these **words are representative** of the speeches used by countries.\n",
    "There are words that are used often by all, however we clustered the groups takin this fact into account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform a principal component analysis to see if some countries use similar words and thus cluster together. To do so, we build a matrix with the words as columns and the rows as the count of these words for each country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the dictionary with the word counts for each country\n",
    "with open ('../data/Emotions/word_dictionary', 'rb') as fp:\n",
    "    itemlist = pickle.load(fp)\n",
    "# open the file with all the 800 words\n",
    "HRE = pd.read_csv(\"../data/Emotions/Human_Readable_Emotions_v2\",sep='\\t')\n",
    "HRE = HRE.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c1.1</th>\n",
       "      <th>c2.7</th>\n",
       "      <th>c2.8</th>\n",
       "      <th>c2.9</th>\n",
       "      <th>c2.10</th>\n",
       "      <th>c2.12</th>\n",
       "      <th>c2.13</th>\n",
       "      <th>c2.16</th>\n",
       "      <th>c2.17</th>\n",
       "      <th>c2.18</th>\n",
       "      <th>...</th>\n",
       "      <th>c36.31</th>\n",
       "      <th>c36.32</th>\n",
       "      <th>c36.33</th>\n",
       "      <th>c37.31</th>\n",
       "      <th>c37.32</th>\n",
       "      <th>c37.33</th>\n",
       "      <th>c41.1</th>\n",
       "      <th>c41.2</th>\n",
       "      <th>c41.3</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 758 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   c1.1  c2.7  c2.8  c2.9  c2.10  c2.12  c2.13  c2.16  c2.17  c2.18 ...   \\\n",
       "0   4.0  10.0   8.0  80.0   19.0   93.0    2.0   10.0   56.0   92.0 ...    \n",
       "1   8.0   6.0   6.0  26.0   14.0   28.0    0.0    9.0   26.0   28.0 ...    \n",
       "\n",
       "   c36.31  c36.32  c36.33  c37.31  c37.32  c37.33  c41.1  c41.2  c41.3       \n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0    1.0    0.0    0.0  0.0  \n",
       "1     0.0     0.0     0.0     0.0     0.0     0.0    0.0    0.0    0.0  0.0  \n",
       "\n",
       "[2 rows x 758 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize matrix with the words as columns and the counts for each country as rows\n",
    "feature_list = HRE.Variable.values\n",
    "zero_data = np.zeros(shape=(0, HRE.shape[0]))\n",
    "df = pd.DataFrame(zero_data, columns=feature_list)\n",
    "n_countries = len(itemlist)\n",
    "countries_list = []\n",
    "for i in range(n_countries):\n",
    "    countries_list.append(itemlist[i][0])\n",
    "    df = df.append(itemlist[i][1], ignore_index=True)\n",
    "df = df.fillna(value= 0)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for the principal component analysis\n",
    "x = df.loc[:, feature_list].values\n",
    "x = StandardScaler().fit_transform(x) # standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>principal1</th>\n",
       "      <th>principal2</th>\n",
       "      <th>principal3</th>\n",
       "      <th>principal4</th>\n",
       "      <th>principal5</th>\n",
       "      <th>principal6</th>\n",
       "      <th>countries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.054989</td>\n",
       "      <td>-0.120160</td>\n",
       "      <td>0.687818</td>\n",
       "      <td>1.187802</td>\n",
       "      <td>0.428953</td>\n",
       "      <td>0.051040</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.434259</td>\n",
       "      <td>-0.196003</td>\n",
       "      <td>-0.106151</td>\n",
       "      <td>-0.063644</td>\n",
       "      <td>-0.083600</td>\n",
       "      <td>-0.067572</td>\n",
       "      <td>Albania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.460121</td>\n",
       "      <td>-0.208953</td>\n",
       "      <td>-0.128601</td>\n",
       "      <td>-0.046756</td>\n",
       "      <td>-0.091875</td>\n",
       "      <td>-0.105244</td>\n",
       "      <td>Angola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.520203</td>\n",
       "      <td>-0.250687</td>\n",
       "      <td>-0.098517</td>\n",
       "      <td>-0.031553</td>\n",
       "      <td>-0.125630</td>\n",
       "      <td>-0.104326</td>\n",
       "      <td>Anguilla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.509191</td>\n",
       "      <td>-0.239283</td>\n",
       "      <td>-0.108023</td>\n",
       "      <td>-0.035506</td>\n",
       "      <td>-0.112824</td>\n",
       "      <td>-0.109143</td>\n",
       "      <td>Argentina</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   principal1  principal2  principal3  principal4  principal5  principal6  \\\n",
       "0   -3.054989   -0.120160    0.687818    1.187802    0.428953    0.051040   \n",
       "1   -3.434259   -0.196003   -0.106151   -0.063644   -0.083600   -0.067572   \n",
       "2   -3.460121   -0.208953   -0.128601   -0.046756   -0.091875   -0.105244   \n",
       "3   -3.520203   -0.250687   -0.098517   -0.031553   -0.125630   -0.104326   \n",
       "4   -3.509191   -0.239283   -0.108023   -0.035506   -0.112824   -0.109143   \n",
       "\n",
       "     countries  \n",
       "0  Afghanistan  \n",
       "1      Albania  \n",
       "2       Angola  \n",
       "3     Anguilla  \n",
       "4    Argentina  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the first 4 principal components\n",
    "pca = PCA(n_components=6)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal1', 'principal2', 'principal3', 'principal4', 'principal5', 'principal6'])\n",
    "# add the countries to the table\n",
    "principalDf['countries'] = countries_list\n",
    "principalDf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As specified in milestone2 :\n",
    "> We see that different variables refer to different feelings.\n",
    "> After some research (as you'll see) one of the most common emotions is \"H4Lvd\" which clearly is not an emotion or a feeling of the speech, but it corresponds to 2 dictionaries. Meaning that these words belong to the Harvard and Lasswell dictionaries. In order to understand what this means, we went to see the spreadsheet of the words in these dictionaries, additional information found in [H4Lvd](http://www.wjh.harvard.edu/~inquirer/spreadsheet_guide.htm).\n",
    "When we don't have the specific emotion, it is useful to know which is the most common feeling associated with the most common dictionaries (therefore the check of the spreadsheets). For some others, we already know the feeling the dictionary refers to, such as \"Positivity\" via Lexicoder, “Smugness” via WordNet Affect, “Passivity” via Regressive Imagery Dictionary, etc. With this information we can associate these sentiments with the news and speeches for each event\n",
    "\n",
    "This was overcome by building our own dictionary (as you saw just now) and refering to those words to make our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 <a id='Part3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mainly do our plots with the plotly library. The choropleth world maps are drawn with GeoJSON.\n",
    "The plots are linked by the html source code and the code is found in the [Visualization script](Visualization.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from Visualization import *\n",
    "import pandas as pd\n",
    "\n",
    "# update when changing functions\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q0. **Where does the news come from?** <a id='Part3_Q0'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the map below we see the number of GDELT online news sources agregated over country, logarithmic scale is used. Whole code [here](domains.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://matterhorn-ada.github.io/urls-log.html\" width=\"100%\" height=\"400px\" frameBorder=\"0\" scrolling=\"no\"></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://matterhorn-ada.github.io/urls-log.html\" width=\"100%\" height=\"400px\" frameBorder=\"0\" scrolling=\"no\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the map below we see the number of events happening over the world agregated over country, logarithmic scale is used. Whole code [here](domains.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://matterhorn-ada.github.io/events-log.html\" width=\"100%\" height=\"400px\" frameBorder=\"0\" scrolling=\"no\"></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://matterhorn-ada.github.io/events-log.html\" width=\"100%\" height=\"400px\" frameBorder=\"0\" scrolling=\"no\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the map below we see the number of reports about events happening over the world agregated over country, logarithmic scale is used. Whole code [here](domains.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://matterhorn-ada.github.io/reports-log.html\" width=\"100%\" height=\"400px\" frameBorder=\"0\" scrolling=\"no\"></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://matterhorn-ada.github.io/reports-log.html\" width=\"100%\" height=\"400px\" frameBorder=\"0\" scrolling=\"no\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. **Are we emotionally biased?** <a id='Part3_Q1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"900\" height=\"800\" frameborder=\"0\" scrolling=\"no\" src=\"//plot.ly/~matterhorn_ada/24.embed\"></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe width=\"900\" height=\"800\" frameborder=\"0\" scrolling=\"no\" src=\"//plot.ly/~matterhorn_ada/24.embed\"></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"900\" height=\"500\" frameborder=\"0\" scrolling=\"no\" src=\"//plot.ly/~matterhorn_ada/6.embed\"></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe width=\"900\" height=\"500\" frameborder=\"0\" scrolling=\"no\" src=\"//plot.ly/~matterhorn_ada/6.embed\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. **Are some countries ignored in the news?** <a id='Part3_Q2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot a bubble plot, where the size of the bubble relates to the population count of the country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"900\" height=\"800\" frameborder=\"0\" scrolling=\"no\" src=\"//plot.ly/~matterhorn_ada/5.embed\"></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe width=\"900\" height=\"800\" frameborder=\"0\" scrolling=\"no\" src=\"//plot.ly/~matterhorn_ada/5.embed\"></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we calculate the correlation coefficient \n",
    "data_q2 = pd.read_csv(\"../data/data_q2_final.csv\") #due to its size this data is not in the Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sum_articles</th>\n",
       "      <th>count_events</th>\n",
       "      <th>country</th>\n",
       "      <th>pop</th>\n",
       "      <th>continent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11894575.0</td>\n",
       "      <td>2710326.0</td>\n",
       "      <td>Iran (Islamic Republic of)</td>\n",
       "      <td>81,162,788</td>\n",
       "      <td>Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1177686.0</td>\n",
       "      <td>240928.0</td>\n",
       "      <td>Morocco</td>\n",
       "      <td>35,739,580</td>\n",
       "      <td>Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>681964.0</td>\n",
       "      <td>127535.0</td>\n",
       "      <td>Belize</td>\n",
       "      <td>374,681</td>\n",
       "      <td>Americas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>451584.0</td>\n",
       "      <td>94845.0</td>\n",
       "      <td>El Salvador</td>\n",
       "      <td>6,377,853</td>\n",
       "      <td>Americas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>611711.0</td>\n",
       "      <td>123985.0</td>\n",
       "      <td>Ecuador</td>\n",
       "      <td>16,624,858</td>\n",
       "      <td>Americas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  sum_articles  count_events                     country  \\\n",
       "0           0    11894575.0     2710326.0  Iran (Islamic Republic of)   \n",
       "1           1     1177686.0      240928.0                     Morocco   \n",
       "2           2      681964.0      127535.0                      Belize   \n",
       "3           3      451584.0       94845.0                 El Salvador   \n",
       "4           4      611711.0      123985.0                     Ecuador   \n",
       "\n",
       "          pop continent  \n",
       "0  81,162,788      Asia  \n",
       "1  35,739,580    Africa  \n",
       "2     374,681  Americas  \n",
       "3   6,377,853  Americas  \n",
       "4  16,624,858  Americas  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_q2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9995895757120425"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_q2.sum_articles.corr(data_q2.count_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pearson correlation coefficient is almost 1, which is expected since the relation between the variables is positively increasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. **Do we have a saturation limit?** <a id='Part3_Q4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"900\" height=\"800\" frameborder=\"0\" scrolling=\"no\" src=\"//plot.ly/~StudentUni/47.embed\"></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe width=\"900\" height=\"800\" frameborder=\"0\" scrolling=\"no\" src=\"//plot.ly/~StudentUni/47.embed\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this image you can see the number of events by country in one month and how they relate to the emotion charge. We can see that in fact there is a saturation as the emotion value for the countries with a lot of events do not go over 0.1, while the emotion charge for the countries in that month that did not have that much events is more variable!\n",
    "After a certain point we do get tired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. **Who is more emotional and what do they say?** <a id='Part3_Q5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~StudentUni/48.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = visualization_Q5(Q5_df,All_dicts_Words,['United States of America','Kenya','Philippines','Mexico','Greece', 'Jamaica'])\n",
    "py.iplot(fig, filename='line-mode', auto_open=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the some of the most commom words used by some countries. The somewhat stable percentages that you see, is due to the fact that we get the events that this words appeared, and not the number of words in that specific event. This means that some words appear as clusters but we can see some difference in speach by some of the countries, which is really awesome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"900\" height=\"500\" frameborder=\"0\" scrolling=\"no\" src=\"//plot.ly/~matterhorn_ada/22.embed\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe width=\"900\" height=\"500\" frameborder=\"0\" scrolling=\"no\" src=\"//plot.ly/~matterhorn_ada/22.embed\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principal component analysis mainly shows that most of the countries form a big cluster, which is explained by the fact that news article are not tremendously different from each other in the word choice. However, there are also some small clusters that form, which shows that some countries use similar, but a little bit different, word choices, but on the other hand there are also some outliers which means, that some countries use very different words than others."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
