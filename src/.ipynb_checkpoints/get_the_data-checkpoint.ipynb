{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyspark\n",
    "import os\n",
    "import scipy as sp\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GKG_SCHEMA = StructType([\n",
    "        StructField(\"GKGRECORDID\",StringType(),True),\n",
    "        StructField(\"DATE\",StringType(),True),\n",
    "        StructField(\"SourceCollectionIdentifier\",StringType(),True),\n",
    "        StructField(\"SourceCommonName\",StringType(),True),\n",
    "        StructField(\"DocumentIdentifier\",StringType(),True),\n",
    "        StructField(\"Counts\",StringType(),True),\n",
    "        StructField(\"V2Counts\",StringType(),True),\n",
    "        StructField(\"Themes\",StringType(),True),\n",
    "        StructField(\"V2Themes\",StringType(),True),\n",
    "        StructField(\"Locations\",StringType(),True),\n",
    "        StructField(\"V2Locations\",StringType(),True),\n",
    "        StructField(\"Persons\",StringType(),True),\n",
    "        StructField(\"V2Persons\",StringType(),True),\n",
    "        StructField(\"Organizations\",StringType(),True),\n",
    "        StructField(\"V2Organizations\",StringType(),True),\n",
    "        StructField(\"V2Tone\",StringType(),True),\n",
    "        StructField(\"Dates\",StringType(),True),\n",
    "        StructField(\"GCAM\",StringType(),True),\n",
    "        StructField(\"SharingImage\",StringType(),True),\n",
    "        StructField(\"RelatedImages\",StringType(),True),\n",
    "        StructField(\"SocialImageEmbeds\",StringType(),True),\n",
    "        StructField(\"SocialVideoEmbeds\",StringType(),True),\n",
    "        StructField(\"Quotations\",StringType(),True),\n",
    "        StructField(\"AllNames\",StringType(),True),\n",
    "        StructField(\"Amounts\",StringType(),True),\n",
    "        StructField(\"TranslationInfo\",StringType(),True),\n",
    "        StructField(\"Extras\",StringType(),True)\n",
    "        ])\n",
    "\n",
    "EVENTS_SCHEMA = StructType([\n",
    "    StructField(\"GLOBALEVENTID\",LongType(),True),\n",
    "    StructField(\"Day_DATE\",StringType(),True),\n",
    "    StructField(\"MonthYear_Date\",StringType(),True),\n",
    "    StructField(\"Year_Date\",StringType(),True),\n",
    "    StructField(\"FractionDate\",FloatType(),True),\n",
    "    StructField(\"Actor1Code\",StringType(),True),\n",
    "    StructField(\"Actor1Name\",StringType(),True),\n",
    "    StructField(\"Actor1CountryCode\",StringType(),True),\n",
    "    StructField(\"Actor1KnownGroupCode\",StringType(),True),\n",
    "    StructField(\"Actor1EthnicCode\",StringType(),True),\n",
    "    StructField(\"Actor1Religion1Code\",StringType(),True),\n",
    "    StructField(\"Actor1Religion2Code\",StringType(),True),\n",
    "    StructField(\"Actor1Type1Code\",StringType(),True),\n",
    "    StructField(\"Actor1Type2Code\",StringType(),True),\n",
    "    StructField(\"Actor1Type3Code\",StringType(),True),\n",
    "    StructField(\"Actor2Code\",StringType(),True),\n",
    "    StructField(\"Actor2Name\",StringType(),True),\n",
    "    StructField(\"Actor2CountryCode\",StringType(),True),\n",
    "    StructField(\"Actor2KnownGroupCode\",StringType(),True),\n",
    "    StructField(\"Actor2EthnicCode\",StringType(),True),\n",
    "    StructField(\"Actor2Religion1Code\",StringType(),True),\n",
    "    StructField(\"Actor2Religion2Code\",StringType(),True),\n",
    "    StructField(\"Actor2Type1Code\",StringType(),True),\n",
    "    StructField(\"Actor2Type2Code\",StringType(),True),\n",
    "    StructField(\"Actor2Type3Code\",StringType(),True),\n",
    "    StructField(\"IsRootEvent\",LongType(),True),\n",
    "    StructField(\"EventCode\",StringType(),True),\n",
    "    StructField(\"EventBaseCode\",StringType(),True),\n",
    "    StructField(\"EventRootCode\",StringType(),True),\n",
    "    StructField(\"QuadClass\",LongType(),True),\n",
    "    StructField(\"GoldsteinScale\",FloatType(),True),\n",
    "    StructField(\"NumMentions\",LongType(),True),\n",
    "    StructField(\"NumSources\",LongType(),True),\n",
    "    StructField(\"NumArticles\",LongType(),True),\n",
    "    StructField(\"AvgTone\",FloatType(),True),\n",
    "    StructField(\"Actor1Geo_Type\",LongType(),True),\n",
    "    StructField(\"Actor1Geo_FullName\",StringType(),True),\n",
    "    StructField(\"Actor1Geo_CountryCode\",StringType(),True),\n",
    "    StructField(\"Actor1Geo_ADM1Code\",StringType(),True),\n",
    "    StructField(\"Actor1Geo_ADM2Code\",StringType(),True),\n",
    "    StructField(\"Actor1Geo_Lat\",FloatType(),True),\n",
    "    StructField(\"Actor1Geo_Long\",FloatType(),True),\n",
    "    StructField(\"Actor1Geo_FeatureID\",StringType(),True),\n",
    "    StructField(\"Actor2Geo_Type\",LongType(),True),\n",
    "    StructField(\"Actor2Geo_FullName\",StringType(),True),\n",
    "    StructField(\"Actor2Geo_CountryCode\",StringType(),True),\n",
    "    StructField(\"Actor2Geo_ADM1Code\",StringType(),True),\n",
    "    StructField(\"Actor2Geo_ADM2Code\",StringType(),True),\n",
    "    StructField(\"Actor2Geo_Lat\",FloatType(),True),\n",
    "    StructField(\"Actor2Geo_Long\",FloatType(),True),\n",
    "    StructField(\"Actor2Geo_FeatureID\",StringType(),True),\n",
    "    StructField(\"ActionGeo_Type\",LongType(),True),\n",
    "    StructField(\"ActionGeo_FullName\",StringType(),True),\n",
    "    StructField(\"ActionGeo_CountryCode\",StringType(),True),\n",
    "    StructField(\"ActionGeo_ADM1Code\",StringType(),True),\n",
    "    StructField(\"ActionGeo_ADM2Code\",StringType(),True),\n",
    "    StructField(\"ActionGeo_Lat\",FloatType(),True),\n",
    "    StructField(\"ActionGeo_Long\",FloatType(),True),\n",
    "    StructField(\"ActionGeo_FeatureID\",StringType(),True),\n",
    "    StructField(\"DATEADDED\",LongType(),True),\n",
    "    StructField(\"SOURCEURL\",StringType(),True)\n",
    "    ])\n",
    "\n",
    "MENTIONS_SCHEMA = StructType([\n",
    "    StructField(\"GLOBALEVENTID\",LongType(),True),\n",
    "    StructField(\"EventTimeDate\",LongType(),True),\n",
    "    StructField(\"MentionTimeDate\",LongType(),True),\n",
    "    StructField(\"MentionType\",LongType(),True),\n",
    "    StructField(\"MentionSourceName\",StringType(),True),\n",
    "    StructField(\"MentionIdentifier\",StringType(),True),\n",
    "    StructField(\"SentenceID\",LongType(),True),\n",
    "    StructField(\"Actor1CharOffset\",LongType(),True),\n",
    "    StructField(\"Actor2CharOffset\",LongType(),True),\n",
    "    StructField(\"ActionCharOffset\",LongType(),True),\n",
    "    StructField(\"InRawText\",LongType(),True),\n",
    "    StructField(\"Confidence\",LongType(),True),\n",
    "    StructField(\"MentionDocLen\",LongType(),True),\n",
    "    StructField(\"MentionDocTone\",FloatType(),True),\n",
    "    StructField(\"MentionDocTranslationInfo\",StringType(),True),\n",
    "    StructField(\"Extras\",StringType(),True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change this in the cluster\n",
    "\n",
    "#DATA_DIR = 'hdfs:///datasets/gdeltv2'\n",
    "DATA_DIR = '../data/'\n",
    "\n",
    "# directory for local files (ex.: UrlToCountry)\n",
    "DATA_LOCAL = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df = spark.read.option(\"sep\", \"\\t\").csv(os.path.join(DATA_DIR, \"*.export.CSV\"),schema=EVENTS_SCHEMA)\n",
    "gkg_df = spark.read.option(\"sep\", \"\\t\").csv(os.path.join(DATA_DIR, \"*.gkg.csv\"),schema=GKG_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4169 3639\n"
     ]
    }
   ],
   "source": [
    "print(gkg_df.count(), events_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the DF as a Parquet file\n",
    "events_df.write.parquet(DATA_DIR + 'events.parquet')\n",
    "gkg_df.write.parquet(DATA_DIR + 'gkg.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data from Parquet file\n",
    "events_df = spark.read.parquet(DATA_DIR + 'events.parquet')\n",
    "gkg_df = spark.read.parquet(DATA_DIR + 'gkg.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+--------------------------+----------------+---------------------------------------------------------------------------------------------------+\n",
      "|GKGRECORDID     |DATE          |SourceCollectionIdentifier|SourceCommonName|Locations                                                                                          |\n",
      "+----------------+--------------+--------------------------+----------------+---------------------------------------------------------------------------------------------------+\n",
      "|20171123073000-0|20171123073000|1                         |nrl.com         |null                                                                                               |\n",
      "|20171123073000-1|20171123073000|1                         |360nobs.com     |1#United States#US#US#39.828175#-98.5795#US;1#Nigeria#NI#NI#10#8#NI;1#United Kingdom#UK#UK#54#-4#UK|\n",
      "+----------------+--------------+--------------------------+----------------+---------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gkg_df.select(\"GKGRECORDID\", \"DATE\", \"SourceCollectionIdentifier\", \"SourceCommonName\", \"Locations\") \\\n",
    "                .filter(gkg_df[\"SourceCollectionIdentifier\"] == '1').show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the gkg data for Q1\n",
    "gkg_Q1 = gkg_df.select(\"GKGRECORDID\", \"DATE\", \"SourceCollectionIdentifier\", \"SourceCommonName\", \"Locations\") \\\n",
    "                .filter(gkg_df[\"SourceCollectionIdentifier\"] == '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o125.load.\n: java.lang.NoSuchMethodError: org.apache.spark.rdd.RDD.mapPartitionsWithIndexInternal$default$3()Z\n\tat org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:89)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:2980)\n\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:2978)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.inferFromDataset(CSVDataSource.scala:164)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:149)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\n\tat scala.Option.orElse(Option.scala:289)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-81d91142f3cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# open the file url to country\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mUrlToCountry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_LOCAL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"UrlToCountry.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o125.load.\n: java.lang.NoSuchMethodError: org.apache.spark.rdd.RDD.mapPartitionsWithIndexInternal$default$3()Z\n\tat org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:89)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:2980)\n\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:2978)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.inferFromDataset(CSVDataSource.scala:164)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:149)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\n\tat scala.Option.orElse(Option.scala:289)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
     ]
    }
   ],
   "source": [
    "# open the file url to country\n",
    "UrlToCountry = spark.read.format(\"csv\").option(\"header\", \"true\").load(DATA_LOCAL + \"UrlToCountry.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+\n",
      "|country_source|              url|\n",
      "+--------------+-----------------+\n",
      "|   Afghanistan|       1tvnews.af|\n",
      "|   Afghanistan|       1tvnews.af|\n",
      "|   Afghanistan|       1tvnews.af|\n",
      "|   Afghanistan|    ariananews.af|\n",
      "|   Afghanistan|    ariananews.af|\n",
      "|   Afghanistan|    ariananews.af|\n",
      "|   Afghanistan|da.azadiradio.com|\n",
      "|   Afghanistan|pa.azadiradio.com|\n",
      "|   Afghanistan|          bbc.com|\n",
      "|   Afghanistan|          bbc.com|\n",
      "|   Afghanistan|           dw.com|\n",
      "|   Afghanistan|           dw.com|\n",
      "|   Afghanistan|     dari.irib.ir|\n",
      "|   Afghanistan|   pashto.irib.ir|\n",
      "|   Afghanistan|        negaah.tv|\n",
      "|   Afghanistan|        nooraf.tv|\n",
      "|   Afghanistan|        rferl.org|\n",
      "|   Afghanistan|           tkg.af|\n",
      "|   Afghanistan|           tkg.af|\n",
      "|   Afghanistan|           tkg.af|\n",
      "+--------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select and rename columns of interest\n",
    "UrlToCountry = UrlToCountry.select(UrlToCountry['Country name'].alias('country_source'), UrlToCountry['Clean URL'].alias('url')) \n",
    "UrlToCountry.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------+--------------------------+--------------------+--------------------+--------------+--------------------+\n",
      "|      GKGRECORDID|          DATE|SourceCollectionIdentifier|    SourceCommonName|           Locations|country_source|                 url|\n",
      "+-----------------+--------------+--------------------------+--------------------+--------------------+--------------+--------------------+\n",
      "| 20171123073000-0|20171123073000|                         1|             nrl.com|                null|          null|                null|\n",
      "| 20171123073000-1|20171123073000|                         1|         360nobs.com|1#United States#U...|          null|                null|\n",
      "| 20171123073000-2|20171123073000|                         1|            ann7.com|4#Durban, Kwazulu...|  South Africa|            ann7.com|\n",
      "| 20171123073000-3|20171123073000|                         1|          openpr.com|1#Japan#JA#JA#36#...|          null|                null|\n",
      "| 20171123073000-4|20171123073000|                         1|      mcxcontrol.com|                null|          null|                null|\n",
      "| 20171123073000-5|20171123073000|                         1|      thetimes.co.uk|1#Ireland#EI#EI#5...|United Kingdom|      thetimes.co.uk|\n",
      "| 20171123073000-6|20171123073000|                         1|    moneycontrol.com|1#United States#U...|         India|    moneycontrol.com|\n",
      "| 20171123073000-7|20171123073000|                         1|            ketv.com|2#New York, Unite...| United States|            ketv.com|\n",
      "| 20171123073000-8|20171123073000|                         1|  weeklyregister.com|1#Chile#CI#CI#-30...|          null|                null|\n",
      "| 20171123073000-9|20171123073000|                         1|  heraldscotland.com|2#Florida, United...|United Kingdom|  heraldscotland.com|\n",
      "|20171123073000-10|20171123073000|                         1|thenorthernecho.c...|4#Northallerton, ...|United Kingdom|thenorthernecho.c...|\n",
      "|20171123073000-11|20171123073000|                         1|    itnewsafrica.com|4#Nairobi, Nairob...|          null|                null|\n",
      "|20171123073000-12|20171123073000|                         1|    lompocrecord.com|1#China#CH#CH#35#...| United States|    lompocrecord.com|\n",
      "|20171123073000-13|20171123073000|                         1|  dailypolitical.com|1#United States#U...|          null|                null|\n",
      "|20171123073000-14|20171123073000|                         1|       wallpaper.com|1#Portugal#PO#PO#...|          null|                null|\n",
      "|20171123073000-15|20171123073000|                         1|   meltontimes.co.uk|4#Haverhill, Suff...|United Kingdom|   meltontimes.co.uk|\n",
      "|20171123073000-16|20171123073000|                         1|     statesville.com|2#New York, Unite...|          null|                null|\n",
      "|20171123073000-17|20171123073000|                         1|          siasat.com|1#Iraq#IZ#IZ#33#4...|         India|          siasat.com|\n",
      "|20171123073000-17|20171123073000|                         1|          siasat.com|1#Iraq#IZ#IZ#33#4...|         India|          siasat.com|\n",
      "|20171123073000-17|20171123073000|                         1|          siasat.com|1#Iraq#IZ#IZ#33#4...|         India|          siasat.com|\n",
      "+-----------------+--------------+--------------------------+--------------------+--------------------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# merge the two dataframes to get the country of the source \n",
    "gkg_Q1_country = gkg_Q1.join(UrlToCountry, UrlToCountry['url'] == gkg_Q1['SourceCommonName'], \"left_outer\") \n",
    "gkg_Q1_country.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract and save urls that have no country \n",
    "unknown_urls = gkg_Q1_country.filter(\"country_source is null\")\\\n",
    "                             .select('SourceCommonName').distinct()\n",
    "unknown_urls.coalesce(1).write.csv(DATA_LOCAL+ \"unknown_urls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out countries with unknown url\n",
    "gkg_Q1_country_clean = gkg_Q1_country.filter(gkg_Q1_country.country_source.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown urls  438\n",
      "known urls 80\n"
     ]
    }
   ],
   "source": [
    "print('unknown urls ', unknown_urls.count())\n",
    "print('known urls', gkg_Q1_country_clean.select('country_source').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------+\n",
      "|Locations                                                                                                                  |\n",
      "+---------------------------------------------------------------------------------------------------------------------------+\n",
      "|null                                                                                                                       |\n",
      "|1#United States#US#US#39.828175#-98.5795#US;1#Nigeria#NI#NI#10#8#NI;1#United Kingdom#UK#UK#54#-4#UK                        |\n",
      "|4#Durban, Kwazulu-Natal, South Africa#SF#SF02#-29.85#31.0167#-1224926                                                      |\n",
      "|1#Japan#JA#JA#36#138#JA;1#United States#US#US#39.828175#-98.5795#US;1#Netherlands#NL#NL#52.5#5.75#NL;1#Spain#SP#SP#40#-4#SP|\n",
      "|null                                                                                                                       |\n",
      "+---------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get location of event\n",
    "gkg_df.select(\"Locations\").show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'hdfs:///datasets/gdeltv2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ONLY on computer\n",
    "DATA_DIR = '../data/'\n",
    "\n",
    "# directory for local files (ex.: UrlToCountry)\n",
    "DATA_LOCAL = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df = spark.read.option(\"sep\", \"\\t\").csv(os.path.join(DATA_DIR, \"*.export.CSV\"),schema=EVENTS_SCHEMA)\n",
    "mentions_df = spark.read.option(\"sep\", \"\\t\").csv(os.path.join(DATA_DIR, \"*.mentions.CSV\"),schema=MENTIONS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o47.load.\n: java.lang.NoSuchMethodError: org.apache.spark.rdd.RDD.mapPartitionsWithIndexInternal$default$3()Z\n\tat org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:89)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:2980)\n\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:2978)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.inferFromDataset(CSVDataSource.scala:164)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:149)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\n\tat scala.Option.orElse(Option.scala:289)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ce6f25403ddf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# open and join the dataframe url to country\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mUrlToCountry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_LOCAL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"UrlToCountry.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#UrlToCountry = UrlToCountry.select(UrlToCountry['Country name'].alias('country_source'), UrlToCountry['Clean URL'].alias('url'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmentions_q1_country\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmentions_q1_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUrlToCountry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUrlToCountry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmentions_q1_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MentionSourceName'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"left_outer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o47.load.\n: java.lang.NoSuchMethodError: org.apache.spark.rdd.RDD.mapPartitionsWithIndexInternal$default$3()Z\n\tat org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:89)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:2980)\n\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:2978)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.inferFromDataset(CSVDataSource.scala:164)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:149)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:63)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:57)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:203)\n\tat scala.Option.orElse(Option.scala:289)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:202)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:393)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
     ]
    }
   ],
   "source": [
    "# select Data from Mentions Dataset\n",
    "mentions_q1_df = mentions_df.select(\"GLOBALEVENTID\", \"EventTimeDate\", \"MentionType\", \"MentionSourceName\") \\\n",
    "                .filter(mentions_df[\"MentionType\"] == '1')\n",
    "\n",
    "# open and join the dataframe url to country\n",
    "UrlToCountry = spark.read.format(\"csv\").option(\"header\", \"true\").load(DATA_LOCAL + \"UrlToCountry.csv\")\n",
    "UrlToCountry = UrlToCountry.select(UrlToCountry['Country name'].alias('country_source'), UrlToCountry['Clean URL'].alias('url')) \n",
    "mentions_q1_country = mentions_q1_df.join(UrlToCountry, UrlToCountry['url'] == mentions_q1_df['MentionSourceName'], \"left_outer\") \n",
    "\n",
    "# print the number of urls that have no country\n",
    "print('number of unknown urls: ', mentions_q1_country.filter(\"country_source is null\").select('SourceCommonName').distinct().count())\n",
    "\n",
    "# filter out urls that are unknown\n",
    "mentions_clean_df = mentions_q1_country.filter(mentions_q1_country.country_source.isNotNull())\n",
    "\n",
    "# print the number of urls associated to a country\n",
    "print('number of known urls', mentions_clean_df.select('country_source').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'events_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e07d3de3a67a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Select Data from Events Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mevents_q1_df\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mevents_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GLOBALEVENTID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ActionGeo_Lat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ActionGeo_Long\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"NumMentions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"NumSources\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"NumArticles\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"AvgTone\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mevents_q1_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevents_q1_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActionGeo_Lat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNotNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'events_df' is not defined"
     ]
    }
   ],
   "source": [
    "# select Data from Events Dataset\n",
    "events_q1_df= events_df.select(\"GLOBALEVENTID\", \"ActionGeo_Lat\", \"ActionGeo_Long\", \"NumMentions\",\"NumSources\",\"NumArticles\",\"AvgTone\")\n",
    "events_q1_df.filter(events_q1_df.ActionGeo_Lat.isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the clean events and mentions dfs\n",
    "event_mentions_df = events_q1_df.join(mentions_q1_df, 'GLOBALEVENTID') \n",
    "event_mentions_df.show(2,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are we emotionally biased? \n",
    "**Do the number of conflicts or their distance from our home define our emotions? Is there an underlying trend of a more positive or negative news perception over time?**\n",
    "\n",
    "Fetch the following:\n",
    "\n",
    "- get the time of the event\n",
    "- get the country of the source (\"SourceCollectionIdentifier\" = 1 (only web), \"SourceCommonName\", GKG)\n",
    "- get the country of the event ('Locations', GKG -> longitude & latitude)\n",
    "- get the average tone of the event (\"AvgTone\", EXPORT)\n",
    "- get the ethnicity ('Actor1EthnicCode', Actor1Religion1Code, Actor1Religion2Code, Actor2EthnicCode', Actor2Religion1Code, Actor2Religion2Code, EXPORT) \n",
    "- Goldstein scale (GoldsteinScale, EXPORT)\n",
    "\n",
    "Analysis:\n",
    "\n",
    "- calculate the distance between the source and the country (get longitude & latitude of every country of the source -> compute euclidean distance)\n",
    "- dependence of average tone and distance\n",
    "- aggregate per country and average over average tone -> dependence between events per country and average tone\n",
    "- aggregate per religion/ethnicity and average over average tone (and/ or Goldstein) -> look at dependence\n",
    "- repeat the analysis in time bins\n",
    "\n",
    "Visualization:\n",
    "\n",
    "Discussion Thursday:\n",
    "- open all the files within 1 hour/ 1 day\n",
    "- count number of mentions an event gets and compare it to the number of mentions given\n",
    "- get the country from the url in mentions\n",
    "- calculate distance between country_source (from capital of country) and event_country (\"ActionGeo_Lat, and Long from EVENTS)\n",
    "- aggregate mentions over countries_source and event_country\n",
    "- MentionDocTone for each source might/will allow to get the emotion of the source country\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are some countries ignored in the news? \n",
    "**Is the number of conflicts taking place in a country in relation with the number of mentions in the media depending on where the conflict has happened?**\n",
    "\n",
    "Fetch the following:\n",
    "\n",
    "- country of the event (see above)\n",
    "- number of mentions (NumMentions, NumSources, NumArticles, EXPORT)\n",
    "\n",
    "Analysis:\n",
    "\n",
    "- aggregate per country and average over the number of mentions (which one?, or maybe all and then see) -> dependence between country and number of records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are we emotionally predictable? \n",
    "**Can we observe patterns of emotions with respect to a country, religion or an ethnical group? Can we derive a model predicting emotions in case of a new conflict based on its specific features?**\n",
    "\n",
    "Fetch the following:\n",
    "\n",
    "- group events by location, religion, ethnicity\n",
    "- average over average tone for this group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
