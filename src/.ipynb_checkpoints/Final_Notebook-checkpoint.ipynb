{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotions in Conflicts: Data Collection, Analysis & Visualization\n",
    "\n",
    "\n",
    "In this notebook we present our pipeline to answer the research questions that we raised in Milestone 1. We start by a part 0, where we present and discuss some general points, mostly technical, that we encountered and considered throughout the project. Then, in the first part, we show our step-step strategy on how we proceed to answer the individual questions. In the second part, the code we used to fetch the data and to do the analysis is provided. In the third part we present our visualizations of the data analysis.\n",
    "\n",
    "In this notebook, the code in Part 2 is executed on a sample of nine datafiles fetched from the cluster (3 for each of the 3 schemas (gkg, mentions, export)) and intermediate outputs are shown to guide the reader through the data processing part. The code used to fetch and process the whole GDELT dataset in the cluster is found in the `src` folder and the individual files are hyperlinked in this notebook. \n",
    "\n",
    "The analysis and the visualization part are done on the data gathered from the whole GDELT dataset, and the detailed interpretations are presented in our [data story](https://matterhorn-ada.github.io)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Part 0](#Part0): General points\n",
    "\n",
    "\n",
    "- [Part 1](#Part1): Strategy to answer the research questions\n",
    "    - [Question 0](#Part1_Q0): Exploratory analysis: Where does the news come from?\n",
    "    - [Question 1](#Part1_Q1): Are we emotionally biased? \n",
    "    - [Question 2](#Part1_Q2): Are some countries ignored in the news?\n",
    "    - [Question 3](#Part1_Q3): Are we emotionally predictable?\n",
    "    - [Question 4](#Part1_Q4): Do we have a saturation limit?\n",
    "    - [Question 5](#Part1_Q5): Who is more emotional?\n",
    "    \n",
    "    \n",
    "- [Part 2](#Part2): Code\n",
    "    - [Question 0](#Part2_Q0): Exploratory Data Fetching \n",
    "    - [Question 1](#Part2_Q1): Fetching & preprocessing of the data   \n",
    "    - [Question 2](#Part2_Q2): Aggregating \n",
    "    - [Question 3](#Part2_Q3a): Fetching & preprocessing of the data (Cluster Code)\n",
    "    - [Question 3](#Part2_Q3b): Decision tree building (learning & validating) \n",
    "    - [Question 4 & 5](#Part2_Q4a): GCAM Emotions data\n",
    "    - [Question 4 & 5](#Part2_Q4b): Emotions dictionary\n",
    "    \n",
    "    \n",
    "- [Part 3](#Part3): Visualization\n",
    "    - [Question 0](#Part3_Q0): Choropleth maps\n",
    "    - [Question 1](#Part3_Q1): Dependency evaluation\n",
    "    - [Question 2](#Part3_Q2): Dependency evaluation\n",
    "    - [Question 4 & 5](#Part3_Q4): Most common emotions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0 <a id='Part0'></a>\n",
    "\n",
    "- All the filtering and aggregation steps of processing the data were executed in the cluster and the resulting dataset were exported as either `parquet` or `json` files. The later processing was done with `Pyspark` and `Pandas` dataframes. \n",
    "\n",
    "\n",
    "- When working with country names and country codes from different data sources (e.g. GDELT country codes, GeoJSON country codes, United Nations country names, ...) several heterogeneities in the names and codes were encountered. GDELT is using the [FIPS country codes](https://en.wikipedia.org/wiki/List_of_FIPS_country_codes) which is mainly used by the US government, whereas the [alpha-2](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-22) letter code introduced International Organization for Standardization is more widely used (example: FIPS code for Switzerland is SZ, and the alpha-2 code is CH). For the processing, we converted the GDELT code to the standard with [this dictionary](add Github CountryCode_FIPS_alpha.csv), taken from [here](https://www.geodatasource.com/resources/tutorials/international-country-code-fips-versus-iso-3166/), however some manuel work had to be done for the country names (some external dataset used did not provide the country code and thus had to be joined on the country name, e.g. the [Human development index](http://hdr.undp.org/en/content/human-development-index-hdi)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 <a id='Part1'></a>\n",
    "\n",
    "### Pursued strategy to answer the research questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 0 <a id='Part1_Q0'></a>\n",
    "\n",
    "**Exploratory analysis: Where does the news come from?**\n",
    "\n",
    "#### Fetching & Processing the data \n",
    "\n",
    " From the GDELT dataset we fetch the following information from the \"Mentions\" and \"Events\" sets:\n",
    "\n",
    "- Url of the article mentioning the source \n",
    "- Location of the event\n",
    "- Location of the country reporting the news: \n",
    "    - Get the country reporting the news from the url of the article using the [GDELT Geographic Source Lookup](https://blog.gdeltproject.org/mapping-the-media-a-geographic-lookup-of-gdelts-sources/)\n",
    "\n",
    "#### Analysis\n",
    "\n",
    "- Where do the events happen?\n",
    "    - Group by the countries and count the number of events happening in this country.\n",
    "\n",
    "- Who reports the news?\n",
    "    - Group by the countries reporting the news and count the number of events reported by each country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 <a id='Part1_Q1'></a>\n",
    "\n",
    "**Are we emotionally biased?** Do the number of conflicts or their distance from our home define our emotions? \n",
    "\n",
    "#### Fetching & Processing the data \n",
    "\n",
    " From the GDELT dataset we fetch the following information from the \"Mentions\" and \"Events\" sets:\n",
    "\n",
    "- Url of the article mentioning the source \n",
    "- Average Tone \n",
    "- Location of the event (latitudinal and longitudinal coordinates)\n",
    "- Number of times the event is mentioned in the news (NumArticles) \n",
    "- Calculation of the distance between the source article and the event: \n",
    "    1. Get the country from the url of the article using the [GDELT Geographic Source Lookup](https://blog.gdeltproject.org/mapping-the-media-a-geographic-lookup-of-gdelts-sources/)\n",
    "    2. Get the geographic coordinates of the capital of the source country using the csv file provided by [this website](http://techslides.com/list-of-countries-and-capitals)\n",
    "    3. Calculate the geographic distance between the source article and the event with the [Great-Circle distance formula](https://en.wikipedia.org/wiki/Great-circle_distance)\n",
    "    \n",
    "    Note that in this approach we approximate the location of the source reporting the article by the capital of the country reporting the news.\n",
    "\n",
    "#### Analysis\n",
    "\n",
    "- Evaluation of the dependency between the emotions and the distance: \n",
    "    1. Plot the emotion metrics (Average Tone) against the distance for each event. Distinguish between the location of the event (colour codes) to see if the country where the event occurred influences the emotion, or the other way around, whether some news from a given country only get reported if they have a negative or positive association. \n",
    "\n",
    "- Evaluation of the dependency between the emotions and the importance of the conflict:\n",
    "    1. Evaluation whether there is a dependency of the emotion metrics and the importance of the conflict using an \"importance of an event\" metrics provided by GDELT (3 very similar metrics are given, NumMentions, NumSources, NumArticles, and we decided to use NumArticles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 <a id='Part1_Q2'></a>\n",
    "\n",
    "**Are some countries ignored in the news?**  Is the number of conflicts taking place in a country in relation with the number of mentions in the media depending on where the conflict has happened? \n",
    "\n",
    "#### Fetching the data \n",
    "\n",
    "We use the same dataset than in Question 1, in addition to the dataset [\"population by country\"](https://en.wikipedia.org/wiki/List_of_countries_by_population_(United_Nations)) provided by the United Nations (2017).\n",
    "\n",
    "#### Analysis\n",
    "\n",
    "- Group by the event country and sum up the number of conflicts and the number of mentions \n",
    "- Evaluate whether there is a correlation between the number of conflicts and mentions or not\n",
    "- Identify countries with a high number of conflicts, but a comparewise low number of mentions\n",
    "- Identify whether the number of people living in a country influence the number of event in a country\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 <a id='Part1_Q3'></a>\n",
    "\n",
    "**Are we emotionally predictable?** Can we observe patterns of emotions with respect to a country, religion or an ethnical group? Can we derive a model predicting emotions in case of a new conflict based on its specific features?\n",
    "\n",
    "# THIS NEEDS TO BE CHANGED\n",
    "\n",
    "#### Rational:\n",
    "\n",
    "To observe patterns of emotions with respect to a country, religion and ethnicity, we will derive a model with the features being the country where the event happened (a selection of max. 10 countries), the religion (around 3), and the ethnicity (around 3) (categorical features), and the response variable being an emotion metrics (average tone, or an emotion from our emotion dictionary). We will then learn this model for each source country (country reporting the event) and validate our approach on an event not included in the training set. Below is an example to illustrate:\n",
    "\n",
    "For every country in the world an “emotion” model is trained on events that happened in Syria, with the actors being of islamic and christianic religion (in the “Events” set we have access to the religion and ethnicity of Actor 1 and Actor 2). Now, if a new conflict with these same features happens, we can predict how e.g. Switzerland or the US (as well as every other country in the world) might react. \n",
    "\n",
    "A further step in observing patterns is then to cluster countries together that having similar emotional responses (with a K-means algorithm). A clustering might give insight about which countries are culturally close to each other. \n",
    "\n",
    "#### Fetching:\n",
    "\n",
    "In addition to the data fetched in Question 1 we select from the “Events” set ethnicity related data (‘Actor1EthnicCode', Actor1Religion1Code, Actor1Religion2Code, Actor2EthnicCode', Actor2Religion1Code, Actor2Religion2Code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 <a id='Part1_Q4'></a>\n",
    "\n",
    "**Do we have a saturation limit?** Does increasing number of conflicts make people feel worse and worse or is there some limit?\n",
    "\n",
    "#### Fetching the data \n",
    "\n",
    "From the GDELT dataset we fetch the following information from the \"GKG\":\n",
    "\n",
    "  - Locations \n",
    "  - V2Tone \n",
    "  - GCAM \n",
    "\n",
    "#### Analysis\n",
    "\n",
    "  - Calculation the increasing number of conflicts: \n",
    "      1. Get the location of the country\n",
    "      2. Parse through the gkg files (in the time interval we wish) and get the events referent to a country.\n",
    "      3. Associate quantity of conflicts with emotions passed\n",
    "\n",
    "  - Possible limit of the emotions: \n",
    "      1. Get the V2Tone and the GCAM feelings referent to the events\n",
    "      2. Evaluate the emotions that we have for each of this event, observing how the media shows the events and if there are some insensibility (measured by lower V2Tone scores and more neutral GCAM feelings) or not after a threshold number of events.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 <a id='Part1_Q5'></a>\n",
    "\n",
    "**Who is more emotional?** Do we see sensitivity differences between some countries or actors? \n",
    "\n",
    "#### Fetching the data \n",
    "\n",
    "From the GDELT dataset we fetch the following information from the \"GKG\" and \"Events\":\n",
    "\n",
    "  - People\n",
    "  - Locations \n",
    "  - V2Tone \n",
    "  - GCAM \n",
    "\n",
    "#### Analysis\n",
    "\n",
    "\n",
    "  - Relation between country and emotion: \n",
    "      1. Similar to question 4\n",
    "      2. Associate specific countries to the emotions. Specifically associate actors (people) to emotions concerning the events related to them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 <a id='Part2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# regular imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from math import radians, sqrt, sin, cos, atan2\n",
    "\n",
    "# function imports\n",
    "from helper_functions import *\n",
    "from Schema import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#import findspark\n",
    "#findspark.init('C:\\opt\\spark')\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import to_timestamp,desc, col, abs, split, count, array, concat_ws, sum, min, max, mean, asc, coalesce, udf, when, window, explode, unix_timestamp, lit\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# scikit learn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "%matplotlib inline\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "#sc = spark.sparkContext\n",
    "\n",
    "# update when changing functions\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_DIR = 'hdfs:///datasets/gdeltv2'  # cluster code\n",
    "DATA_DIR = '../data/' # local code\n",
    "\n",
    "# directory for local files \n",
    "DATA_LOCAL = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open GDELT data\n",
    "gkg_df = spark.read.option(\"sep\", \"\\t\").csv(os.path.join(DATA_DIR, \"*.gkg.csv\"),schema=GKG_SCHEMA)\n",
    "events_df = spark.read.option(\"sep\", \"\\t\").csv(os.path.join(DATA_DIR, \"*.export.CSV\"),schema=EVENTS_SCHEMA)\n",
    "mentions_df = spark.read.option(\"sep\", \"\\t\").csv(os.path.join(DATA_DIR, \"*.mentions.CSV\"),schema=MENTIONS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open helper datasets\n",
    "CountryToCapital = spark.read.format(\"csv\").option(\"header\", \"true\").load(DATA_LOCAL + \"country-capitals.csv\")\n",
    "Domains = spark.read.format(\"csv\").option(\"header\", \"true\").load(DATA_LOCAL + \"urls.csv\")\n",
    "Domains = Domains.select(Domains['alpha-2'].alias('code') ,Domains['name'].alias('country_source'), 'region', Domains['web'].alias('url'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+------+-----------------+\n",
      "|code|country_source|region|              url|\n",
      "+----+--------------+------+-----------------+\n",
      "|  AF|   Afghanistan|  Asia| 108worldnews.com|\n",
      "|  AF|   Afghanistan|  Asia|           1tv.af|\n",
      "|  AF|   Afghanistan|  Asia|       1tvnews.af|\n",
      "|  AF|   Afghanistan|  Asia|       aff.org.af|\n",
      "|  AF|   Afghanistan|  Asia|afghan-review.com|\n",
      "+----+--------------+------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Domains.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "urls linking the url of the news article to the country which is reporting the event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q0.  Exploratory Data Fetching <a id='Part2_Q0'></a>\n",
    "\n",
    "# CHANGE ME The whole cluster code is found [here](github balbalb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+\n",
      "|ActionGeo_CountryCode|count|\n",
      "+---------------------+-----+\n",
      "|                   CI|   15|\n",
      "|                   FI|    1|\n",
      "|                   IC|    1|\n",
      "|                   RO|    9|\n",
      "|                   SL|    7|\n",
      "+---------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### get the number of events grouped by the country they occurred\n",
    "\n",
    "# select the required data from Event Dataset\n",
    "events_1 = events_df.select('ActionGeo_CountryCode').filter(events_df.ActionGeo_CountryCode.isNotNull())\n",
    "CountryEvent_number = events_1.groupBy('ActionGeo_CountryCode').count()\n",
    "CountryEvent_number.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|     country_source|count|\n",
      "+-------------------+-----+\n",
      "|Korea (Republic of)|   36|\n",
      "|        Philippines|  166|\n",
      "|           Malaysia|  151|\n",
      "|               Fiji|    5|\n",
      "|             Turkey|   40|\n",
      "+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### get the number of events grouped by the country reporting\n",
    "\n",
    "# select the required data from Mentions Dataset\n",
    "mentions_1 = mentions_df.select(\"GLOBALEVENTID\", \"MentionType\", \"MentionSourceName\") \\\n",
    "                .filter(mentions_df[\"MentionType\"] == '1') # MentionType 1 refers to the web articles and not e.g. books\n",
    "\n",
    "# join the domain df with the country corresponding to the source\n",
    "mentions_2 = mentions_1.join(Domains, Domains['url'] == mentions_1['MentionSourceName'], \"left_outer\")\n",
    "\n",
    "# filter out urls that are unknown\n",
    "mentions_3 = mentions_2.filter(mentions_2.country_source.isNotNull())\n",
    "\n",
    "events_1 = events_df.select('GLOBALEVENTID')\n",
    "\n",
    "mention_event = events_1.join(mentions_3, 'GLOBALEVENTID')\n",
    "CountrySource_number = mention_event.groupBy('country_source').count()\n",
    "CountrySource_number.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1.  Fetching & preprocessing of the data  <a id='Part2_Q1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHANGE ME The whole cluster code is found [here](github balbalb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unknown urls:  12\n",
      "number of known urls:  843\n"
     ]
    }
   ],
   "source": [
    "# select the required data from Mentions Dataset\n",
    "\n",
    "# select the required data from Mentions Dataset\n",
    "mentions_1 = mentions_df.select(\"GLOBALEVENTID\", \"EventTimeDate\", 'MentionIdentifier', \"MentionType\", \"MentionSourceName\") \\\n",
    "                .filter(mentions_df[\"MentionType\"] == '1')\n",
    "\n",
    "# join the domain df with the country corresponding to the source\n",
    "mentions_2 = mentions_1.join(Domains, Domains['url'] == mentions_1['MentionSourceName'], \"left_outer\")\n",
    "\n",
    "# filter out urls that are unknown\n",
    "mentions_3 = mentions_2.filter(mentions_2.country_source.isNotNull())\n",
    "\n",
    "# print the number of urls that have no country\n",
    "print('number of unknown urls: ', mentions_2.filter(\"url is null\").select('MentionSourceName').distinct().count())\n",
    "\n",
    "# print the number of urls associated to a country\n",
    "print('number of known urls: ', mentions_3.select('url').distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only a small fraction of 'news urls' are unknown in terms of which country they belong to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------+-----------+-----------+----------+-----------+\n",
      "|GLOBALEVENTID|   MentionIdentifier| EventTimeDate|CountryCode|CountryName|Source_Lat|Source_Long|\n",
      "+-------------+--------------------+--------------+-----------+-----------+----------+-----------+\n",
      "|    709307290|http://www.khaama...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|\n",
      "|    709307290|http://www.khaama...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|\n",
      "|    709307289|http://www.khaama...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|\n",
      "|    709307289|http://www.khaama...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|\n",
      "|    709306810|https://www.pajhw...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|\n",
      "|    709306810|https://www.pajhw...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|\n",
      "|    709306809|https://www.pajhw...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|\n",
      "|    709306809|https://www.pajhw...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|\n",
      "|    709306808|https://www.pajhw...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|\n",
      "|    709306808|https://www.pajhw...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|\n",
      "|    709306806|https://www.pajhw...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|\n",
      "|    709306806|https://www.pajhw...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|\n",
      "|    709306805|https://www.pajhw...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|\n",
      "|    709306805|https://www.pajhw...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|\n",
      "|    709319061|http://www.angop....|20171123080000|         AO|     Angola| -8.833333|  13.216667|\n",
      "|    709319061|http://www.angop....|20171123080000|         AO|     Angola| -8.833333|  13.216667|\n",
      "|    709319060|http://www.angop....|20171123080000|         AO|     Angola| -8.833333|  13.216667|\n",
      "|    709319060|http://www.angop....|20171123080000|         AO|     Angola| -8.833333|  13.216667|\n",
      "|    709319059|http://www.angop....|20171123080000|         AO|     Angola| -8.833333|  13.216667|\n",
      "|    709319059|http://www.angop....|20171123080000|         AO|     Angola| -8.833333|  13.216667|\n",
      "+-------------+--------------------+--------------+-----------+-----------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join the file with the countries and capitals\n",
    "mentions_4 = mentions_3.join(CountryToCapital, CountryToCapital['CountryName'] == mentions_3['country_source'], \"left_outer\") \n",
    "\n",
    "# filter out rows with no geographic coordinates\n",
    "mentions_5 = mentions_4.filter(mentions_4.CapitalLatitude.isNotNull())\n",
    "\n",
    "# select relevant columns \n",
    "mentions_6 = mentions_5.select('GLOBALEVENTID', 'MentionIdentifier','EventTimeDate','CountryCode', 'CountryName', mentions_5['CapitalLatitude'].alias('Source_Lat'),\n",
    "                                                   mentions_5['CapitalLongitude'].alias('Source_Long'))\n",
    "mentions_6 = mentions_6.withColumn(\"Source_Lat\", mentions_6[\"Source_Lat\"].cast(\"float\"))\n",
    "mentions_6 = mentions_6.withColumn(\"Source_Long\", mentions_6[\"Source_Long\"].cast(\"float\"))\n",
    "mentions_6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare gkg file\n",
    "gkg_1 = gkg_df.select('DocumentIdentifier', 'V2Tone')\n",
    "split_col = split(gkg_df['V2Tone'], ',')\n",
    "gkg_2 = gkg_1.withColumn('Tone', split_col.getItem(0))\n",
    "gkg_3 = gkg_2.withColumn('Polarity', split_col.getItem(3))\n",
    "gkg_4 = gkg_3.withColumn('Emotion_Charge', abs(gkg_3.Tone - gkg_3.Polarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of events:  5251\n",
      "Number of events with geographic coordinates:  5095\n",
      "+-------------+---------------------+-------------+--------------+-----------+----------+-----------+----------+--------------------+--------------+-----------+-----------+----------+-----------+--------------------+-----------------+\n",
      "|GLOBALEVENTID|ActionGeo_CountryCode|ActionGeo_Lat|ActionGeo_Long|NumMentions|NumSources|NumArticles|   AvgTone|   MentionIdentifier| EventTimeDate|CountryCode|CountryName|Source_Lat|Source_Long|  DocumentIdentifier|   Emotion_Charge|\n",
      "+-------------+---------------------+-------------+--------------+-----------+----------+-----------+----------+--------------------+--------------+-----------+-----------+----------+-----------+--------------------+-----------------+\n",
      "|    709307290|                   AF|      34.5167|       69.1833|          6|         1|          6|-3.5294118|http://www.khaama...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|http://www.khaama...| 9.81595092024539|\n",
      "|    709307290|                   AF|      34.5167|       69.1833|          6|         1|          6|-3.5294118|http://www.khaama...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|http://www.khaama...| 9.81595092024539|\n",
      "|    709307289|                   AF|      34.5167|       69.1833|          4|         1|          4|-3.5294118|http://www.khaama...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|http://www.khaama...| 9.81595092024539|\n",
      "|    709307289|                   AF|      34.5167|       69.1833|          4|         1|          4|-3.5294118|http://www.khaama...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|http://www.khaama...| 9.81595092024539|\n",
      "|    709306810|                   IN|      30.6456|       76.3825|          6|         1|          6| 2.4590163|https://www.pajhw...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|https://www.pajhw...| 0.82304526748971|\n",
      "|    709306810|                   IN|      30.6456|       76.3825|          6|         1|          6| 2.4590163|https://www.pajhw...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|https://www.pajhw...| 0.82304526748971|\n",
      "|    709306809|                   IN|      30.6456|       76.3825|          1|         1|          1| 2.4590163|https://www.pajhw...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|https://www.pajhw...| 0.82304526748971|\n",
      "|    709306809|                   IN|      30.6456|       76.3825|          1|         1|          1| 2.4590163|https://www.pajhw...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|https://www.pajhw...| 0.82304526748971|\n",
      "|    709306808|                   IN|      30.6456|       76.3825|          1|         1|          1| 2.4590163|https://www.pajhw...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|https://www.pajhw...| 0.82304526748971|\n",
      "|    709306808|                   IN|      30.6456|       76.3825|          1|         1|          1| 2.4590163|https://www.pajhw...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|https://www.pajhw...| 0.82304526748971|\n",
      "|    709306806|                   IN|      31.6331|       74.8656|          1|         1|          1| 2.4590163|https://www.pajhw...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|https://www.pajhw...| 0.82304526748971|\n",
      "|    709306806|                   IN|      31.6331|       74.8656|          1|         1|          1| 2.4590163|https://www.pajhw...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|https://www.pajhw...| 0.82304526748971|\n",
      "|    709306805|                   IN|      30.6456|       76.3825|          1|         1|          1| 2.4590163|https://www.pajhw...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|https://www.pajhw...| 0.82304526748971|\n",
      "|    709306805|                   IN|      30.6456|       76.3825|          1|         1|          1| 2.4590163|https://www.pajhw...|20171123064500|         AF|Afghanistan| 34.516666|  69.183334|https://www.pajhw...| 0.82304526748971|\n",
      "|    709319061|                   KN|         40.0|         127.0|         10|         1|         10|-2.9268293|http://www.angop....|20171123080000|         AO|     Angola| -8.833333|  13.216667|http://www.angop....|6.896551724137931|\n",
      "|    709319061|                   KN|         40.0|         127.0|         10|         1|         10|-2.9268293|http://www.angop....|20171123080000|         AO|     Angola| -8.833333|  13.216667|http://www.angop....|6.896551724137931|\n",
      "|    709319060|                   AO|     -8.83833|       13.2344|          2|         1|          2|-2.9268293|http://www.angop....|20171123080000|         AO|     Angola| -8.833333|  13.216667|http://www.angop....|6.896551724137931|\n",
      "|    709319060|                   AO|     -8.83833|       13.2344|          2|         1|          2|-2.9268293|http://www.angop....|20171123080000|         AO|     Angola| -8.833333|  13.216667|http://www.angop....|6.896551724137931|\n",
      "|    709319059|                   AO|     -8.83833|       13.2344|          2|         1|          2|-2.9268293|http://www.angop....|20171123080000|         AO|     Angola| -8.833333|  13.216667|http://www.angop....|6.896551724137931|\n",
      "|    709319059|                   AO|     -8.83833|       13.2344|          2|         1|          2|-2.9268293|http://www.angop....|20171123080000|         AO|     Angola| -8.833333|  13.216667|http://www.angop....|6.896551724137931|\n",
      "+-------------+---------------------+-------------+--------------+-----------+----------+-----------+----------+--------------------+--------------+-----------+-----------+----------+-----------+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select Data from Events Dataset\n",
    "events_1= events_df.select(\"GLOBALEVENTID\", 'ActionGeo_CountryCode', \"ActionGeo_Lat\", \"ActionGeo_Long\", \"NumMentions\",\"NumSources\",\"NumArticles\",\"AvgTone\")\n",
    "\n",
    "# filter out events that have no geographic coordinates\n",
    "print('Total number of events: ', events_1.count())\n",
    "events_2 = events_1.filter(events_1.ActionGeo_Lat.isNotNull())\n",
    "print('Number of events with geographic coordinates: ', events_2.count())\n",
    "\n",
    "# merge the clean events and mentions dfs\n",
    "event_mention = events_2.join(mentions_6, 'GLOBALEVENTID') \n",
    "\n",
    "data_q1 = event_mention.join(gkg_4.select('DocumentIdentifier', 'Emotion_Charge'), gkg_4['DocumentIdentifier'] == event_mention['MentionIdentifier'])\n",
    "data_q1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the events reported by GDELT have a reported geographic location.\n",
    "\n",
    "# CHANGE ME The following part of the code is found [here]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------+-----------+-----------+-------------+\n",
      "|ActionGeo_CountryCode| distance|    AvgTone|NumArticles|SourceCountry|\n",
      "+---------------------+---------+-----------+-----------+-------------+\n",
      "|                   AS|1786.5924|  1.2285012|        3.0|    Australia|\n",
      "|                   AS|1786.5924|  1.2285012|        3.0|    Australia|\n",
      "|                   IN|1087.6713|-0.29154518|        3.0|        India|\n",
      "|                   IN|1087.6713|-0.29154518|        3.0|        India|\n",
      "|                   PE|10281.417|  -6.508876|        2.0|  Netherlands|\n",
      "+---------------------+---------+-----------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# append a column with the geographic distance\n",
    "def geocalc(lat1, lon1, lat2, lon2):\n",
    "    #print(lat1, lon1, lat2, lon2)\n",
    "    lat1 = radians(lat1)\n",
    "    lon1 = radians(lon1)\n",
    "    lat2 = radians(lat2)\n",
    "    lon2 = radians(lon2)\n",
    "    \n",
    "    dlon = lon1 - lon2\n",
    "\n",
    "    EARTH_R = 6372.8\n",
    "\n",
    "    y = sqrt((cos(lat2) * sin(dlon)) ** 2 + (cos(lat1) * sin(lat2) - sin(lat1) * cos(lat2) * cos(dlon)) ** 2)\n",
    "    x = sin(lat1) * sin(lat2) + cos(lat1) * cos(lat2) * cos(dlon)\n",
    "    c = atan2(y, x)\n",
    "    return EARTH_R * c\n",
    "\n",
    "udf_geocalc = udf(geocalc, FloatType())\n",
    " \n",
    "data_q1 = spark.read.parquet(DATA_LOCAL + 'data_q1.parquet')\n",
    "data_q1 = data_q1.withColumn(\"distance\", lit(udf_geocalc('ActionGeo_Lat', 'ActionGeo_Long', 'Source_Lat', 'Source_Long')))\n",
    "\n",
    "data_q1 = data_q1.select('ActionGeo_CountryCode', 'distance', 'AvgTone','NumArticles', data_q1['CountryName'].alias('SourceCountry'))\n",
    "data_q1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applying the algorithm on the whole GDELT dataset, a dataset of 3.4 GB was obtained. Since the goal was to plot each event as a datapoint in a graph, this dataset was randomly downsampled and only 0.01 % were kept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2. Aggregation <a id='Part2_Q2'></a>\n",
    "\n",
    "# CHANGE ME The whole cluster code is found [here](github balbalb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------------+------------+\n",
      "|ActionGeo_CountryCode|sum_articles|count_events|\n",
      "+---------------------+------------+------------+\n",
      "|                   CI|         111|          15|\n",
      "|                   FI|           4|           1|\n",
      "|                   IC|           2|           1|\n",
      "|                   RO|          23|           9|\n",
      "|                   SL|          31|           7|\n",
      "+---------------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_q2 = events_2.groupBy('ActionGeo_CountryCode').agg(sum('NumArticles').alias('sum_articles'), count('GLOBALEVENTID').alias('count_events'))\n",
    "data_q2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. **Are we emotionally predictable?** <a id='Part2_Q3a'></a>\n",
    "\n",
    "### Fetching the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------+------------+----------+-------+\n",
      "|_c0|    Country|      HDI|Country name|FIPS_GDELT|alpha-2|\n",
      "+---+-----------+---------+------------+----------+-------+\n",
      "|  0|     Norway|Very High|      Norway|        NO|     NO|\n",
      "|  1|Switzerland|Very High| Switzerland|        SZ|     CH|\n",
      "|  2|  Australia|Very High|   Australia|        AS|     AU|\n",
      "|  3|    Ireland|Very High|     Ireland|        EI|     IE|\n",
      "|  4|    Germany|Very High|     Germany|        GM|     DE|\n",
      "+---+-----------+---------+------------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# open helper dataset\n",
    "HDI_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../data/HDI_code_df.csv\")\n",
    "HDI_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The human development index is divided into 4 categories: Very High, High, Medium, Low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter on events that have count information\n",
    "gkg_1 = gkg_df.filter(gkg_df.Counts.isNotNull())\n",
    "CountType = split(gkg_1['Counts'], '#')\n",
    "\n",
    "# add the event type\n",
    "gkg_2 = gkg_1.withColumn('EventType', CountType.getItem(0))\n",
    "\n",
    "# add the number of people concerned\n",
    "gkg_3 = gkg_2.withColumn('NumPeople', CountType.getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare mention file\n",
    "mentions_1 = mentions_df.select(\"GLOBALEVENTID\", \"MentionType\", \"MentionSourceName\", 'MentionIdentifier') \\\n",
    "                .filter(mentions_df[\"MentionType\"] == '1')\n",
    "\n",
    "# join the dataframe url to country\n",
    "mentions_2 = mentions_1.join(Domains, Domains['url'] == mentions_1['MentionSourceName'], \"left_outer\") \n",
    "\n",
    "# filter out urls that are unknown\n",
    "mentions_3 = mentions_2.filter(mentions_2.country_source.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_1= events_df.select(\"GLOBALEVENTID\", 'Day_DATE','Actor1Religion1Code', 'Actor2Religion1Code',\n",
    "                               'NumMentions',\"AvgTone\", 'GoldsteinScale', 'ActionGeo_CountryCode')\n",
    "\n",
    "# filter events with no country code\n",
    "events_2 = events_1.filter(events_1.ActionGeo_CountryCode.isNotNull())\n",
    "\n",
    "# create religion column (take one of the two actors, because in the example test there was never data for both at the same time)\n",
    "events_3 = events_2.withColumn('ActorReligion', coalesce(events_2['Actor1Religion1Code'], events_2['Actor2Religion1Code']))\n",
    "\n",
    "# filter out columns with no religion\n",
    "events_4 = events_3.filter(events_3.ActorReligion.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### join the files togther\n",
    "\n",
    "# join the HDI file to country code\n",
    "events_5 = events_4.join(HDI_df.select('Country', 'HDI', 'FIPS_GDELT'), HDI_df['FIPS_GDELT']==events_4['ActionGeo_CountryCode'])\n",
    "\n",
    "# join event and mention file\n",
    "mention_event = events_5.join(mentions_3, 'GLOBALEVENTID')\n",
    "\n",
    "# join mention_event and gkg\n",
    "gkg_mention_event = mention_event.join(gkg_3, mention_event['MentionIdentifier'] == gkg_3['DocumentIdentifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gkg, original size:  6098\n",
      "gkg, filtered:  772\n",
      "mention, original size:  13508\n",
      "mention, filtered:  13421\n",
      "event, original size:  5251\n",
      "event, filtered:  199\n",
      "files joined together:  200\n",
      "all:  49\n"
     ]
    }
   ],
   "source": [
    "print('gkg, original size: ', gkg_df.count())\n",
    "print('gkg, filtered: ', gkg_3.count())\n",
    "print('mention, original size: ', mentions_df.count())\n",
    "print('mention, filtered: ', mentions_3.count())\n",
    "print('event, original size: ', events_df.count())\n",
    "print('event, filtered: ', events_4.count())\n",
    "print('files joined together: ', mention_event.count())\n",
    "print('all: ', gkg_mention_event.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the filtering is reduces the size of the datafiles quite drastically. This is because in the gkg file, most of the articles do not have a specified event type, in the event file, very few events have the religion of the actors defined, and in the mention file, not every article can be linked to a country source reporting the article.\n",
    "\n",
    "In the following we will select the emotions of interest from the GCAM data column available in the gkg file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "HRE = ['c2.152', 'c2.214', 'c3.2', 'c5.7', 'c6.5', 'c7.2', 'c10.1',\n",
    "       'c14.9', 'c15.3', 'c15.4', 'c15.12', 'c15.26', 'c15.27', 'c15.30',\n",
    "       'c15.36', 'c15.42', 'c15.53', 'c15.57', 'c15.61', 'c15.92',\n",
    "       'c15.93', 'c15.94', 'c15.97', 'c15.101', 'c15.102', 'c15.103',\n",
    "       'c15.105', 'c15.106', 'c15.107', 'c15.108', 'c15.109', 'c15.110',\n",
    "       'c15.116', 'c15.120', 'c15.123', 'c15.126', 'c15.131', 'c15.136',\n",
    "       'c15.137', 'c15.152', 'c15.171', 'c15.173', 'c15.179', 'c15.203',\n",
    "       'c15.219', 'c15.221', 'c15.239', 'c15.260', 'c21.1', 'c35.31',\n",
    "       'c24.1', 'c24.2', 'c24.3', 'c24.4', 'c24.5', 'c24.6', 'c24.7',\n",
    "       'c24.8', 'c24.9', 'c24.10', 'c24.11', 'c36.31', 'c37.31', 'c41.1']\n",
    "\n",
    "\n",
    "Emot_Words_df = gkg_mention_event.select(gkg_mention_event['ActionGeo_CountryCode'].alias('CountryEvent'), 'EventType',\n",
    "                                   'ActorReligion', 'HDI', 'AvgTone',\n",
    "                                  'country_source', 'GCAM',split(col(\"GCAM\"), \":\").alias(\"GCAM2\"))\n",
    "\n",
    "Emot_Words_df = Emot_Words_df.withColumn('GCAM2', concat_ws(',', 'GCAM2'))\n",
    "\n",
    "Emot_Words_df = Emot_Words_df.select('CountryEvent', 'EventType','ActorReligion', 'country_source',\n",
    "                                     'HDI', 'AvgTone', split(col(\"GCAM2\"), \",\").alias(\"GCAM\"))\n",
    "\n",
    "Emot_Words_df = Emot_Words_df.withColumn(\"HRE\", array([lit(x) for x in HRE]) )\n",
    "\n",
    "differencer = udf(lambda x,y: list(set(y)-(set(y)-set(x))), ArrayType(StringType()))\n",
    "Emot_Words_df = Emot_Words_df.withColumn('DIF', differencer('HRE', 'GCAM'))\n",
    "\n",
    "Emot_Words_df = Emot_Words_df.select('CountryEvent', 'EventType','ActorReligion', 'country_source',\n",
    "                                     'HDI', 'AvgTone', 'DIF')\n",
    "\n",
    "data_q3 = Emot_Words_df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------------+--------------------+------+----------+--------------------+\n",
      "|CountryEvent|EventType|ActorReligion|      country_source|   HDI|   AvgTone|                 DIF|\n",
      "+------------+---------+-------------+--------------------+------+----------+--------------------+\n",
      "|          BK|     KILL|          MOS|United States of ...|  High|-10.911809|[c15.110, c15.103...|\n",
      "|          MR|  PROTEST|          MOS|              Turkey|   Low|-8.5561495|[c2.214, c41.1, c...|\n",
      "|          ID|   AFFECT|          MOS|            Malaysia|Medium|-2.7181687|[c2.214, c14.9, c...|\n",
      "|          PK|  PROTEST|          MOS|         Philippines|Medium|-3.1468532|[c15.110, c14.9, ...|\n",
      "|          IZ|  PROTEST|          MOS|        Saudi Arabia|Medium| 1.9607843|[c15.171, c2.214,...|\n",
      "+------------+---------+-------------+--------------------+------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_q3.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset serves to build a decision tree in the following. Due to the drastic filtering processing, the algorithm applied on the whole GDELT dataset returned a dataset with a manageable size of 88 MB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the decision tree <a id='Part2_Q3b'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction model:\n",
    "\n",
    "Decision tree model with the Average Tone as the response.\n",
    "\n",
    "Features are: CountryEvent, EventType, ActorReligion, HDI\n",
    "\n",
    "Other considered Features are: NumPeople, ActorEthnicity, GoldsteinScale\n",
    "\n",
    "#### Feature description:\n",
    "\n",
    "CountryEvent: Geographic country where the event has happened (categorical variable). To have a large sample size for each category, we only pick the top 10 countries, that are mentioned most often.\n",
    "\n",
    "EventType: This reflects the type of the event. Categorical value with the following values: AFFECT, ARREST, KIDNAP, KILL, PROTEST, SEIZE, or WOUND.\n",
    "\n",
    "NumPeople: Number of people concerned by the EventType. NOT INCLUDED BECAUSE AS A STANDALONE IT HAS NO MEANING. COULD BE REPLACED BY NUMMENTIONS IF THAT IS FOUND TO BE SIGNIFICANT\n",
    "\n",
    "ActorEthnicity: Ethnicity of the actors implied in the event and indicated by the CAMEO Ethnic Coding Scheme (see Chapter 5 from the [CAMEO\n",
    "Conflict and Mediation Event Observations Event and Actor Codebook](http://data.gdeltproject.org/documentation/CAMEO.Manual.1.1b3.pdf)). We use the top 10 most common ethnic groups (categorical variable). NOT INCLUDED BECAUSE IT IS REPETITIVE OF THE COUNTRYEVENT.\n",
    "\n",
    "ActorReligion: Religion of the actors implied in the event and indicated by the CAMEO Religious Coding Scheme (see Chapter 4 from the [CAMEO\n",
    "Conflict and Mediation Event Observations Event and Actor Codebook](http://data.gdeltproject.org/documentation/CAMEO.Manual.1.1b3.pdf)). 19 religions are reported (categorical variable).\n",
    "\n",
    "GoldsteinScale: Score that reflects the potential impact an event will have on the stability of a country. The attributed value lies between -10 and 10, where a positive value indicates more cooperation than conflict. NOT INCLUDED BECAUSE IT LOOKED THIS VARIABLE IS NOT RELATED TO THE OUTCOME VARIABLE AND THE SITUATION\n",
    "\n",
    "HDI: Human Development Index. This index takes into account the development of a country, not  only including its economic growth (measured by the gross national income), but also life expectance and education (more information are found on the [United Nations Development Website](http://hdr.undp.org/en/content/human-development-index-hdi). Continuous variable (HDI $\\in [0,1]$). We use the [latest version](http://hdr.undp.org/en/composite/HDI) (released in Sep. 2018) covering the period of 2017. \n",
    "\n",
    "AvgTone: The score ranges from -100 (extremely negative) to +100 (extremely positive). Common values range between -10 and +10, with 0 indicating neutral.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_data = Data_q3.toPandas()\n",
    "model_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discretize the average tone\n",
    "bins = pd.IntervalIndex.from_tuples([(-101, -10), (-10, -8), (-8,-5), (-5, -2), (-2, 0), (0,2),\n",
    "                                    (2,5), (5,10), (10, 101)])\n",
    "model_data.AvgTone = pd.cut(model_data.AvgTone, bins)\n",
    "\n",
    "# transform into categorical data & create corresponding dictionary\n",
    "le_CountryEvent = LabelEncoder()\n",
    "model_data['CountryEvent'] = le_CountryEvent.fit_transform(model_data['CountryEvent'])\n",
    "le_CountryEvent_mapping = dict(zip(le_CountryEvent.classes_, le_CountryEvent.transform(le_CountryEvent.classes_)))\n",
    "\n",
    "\n",
    "# transform back with: list(le_CountryEvent.inverse_transform([2, 2, 1]))\n",
    "le_EventType = LabelEncoder()\n",
    "model_data['EventType'] = le_EventType.fit_transform(model_data['EventType'])\n",
    "le_EventType_mapping = dict(zip(le_EventType.classes_, le_EventType.transform(le_EventType.classes_)))\n",
    "\n",
    "le_ActorReligion = LabelEncoder()\n",
    "model_data['ActorReligion'] = le_ActorReligion.fit_transform(model_data['ActorReligion'])\n",
    "le_ActorReligion_mapping = dict(zip(le_ActorReligion.classes_, le_ActorReligion.transform(le_ActorReligion.classes_)))\n",
    "\n",
    "le_HDI = LabelEncoder()\n",
    "model_data['HDI'] = le_HDI.fit_transform(model_data['HDI'])\n",
    "le_HDI_mapping = dict(zip(le_HDI.classes_, le_HDI.transform(le_HDI.classes_)))\n",
    "\n",
    "le_AvgTone\t = LabelEncoder()\n",
    "model_data['AvgTone'] = le_AvgTone.fit_transform(model_data['AvgTone'])\n",
    "le_AvgTone_mapping = dict(zip(le_AvgTone.classes_, le_AvgTone.transform(le_AvgTone.classes_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show transformed dataset\n",
    "model_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X and Y\n",
    "X = model_data.values[:, 0:4]\n",
    "Y = model_data.values[:,4]\n",
    "Y = Y.astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy is \"), accuracy_score(y_test,y_pred)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict for: ['MY', 'AFFECT', 'CHR', 'Very High']\n",
    "le_EventType_mapping['KILL']\n",
    "y_pr =clf.predict([[le_CountryEvent_mapping['MY'], le_EventType_mapping['AFFECT'], le_ActorReligion_mapping['CHR'], le_HDI_mapping['Very High']]])\n",
    "AvTone = [key  for (key, value) in le_AvgTone_mapping.items() if value == y_pr]\n",
    "AvTone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. **Do we have a saturation limit?** / Q5. **Who is more emotional?**\n",
    "\n",
    "The data to answer questions 4 % 5 is pretty similar, therefore the processing for both parts is shown here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4. a) GCAM Emotions data <a id='Part2_Q4a'></a>\n",
    "> We will select the ID ,the emotions, locations, persons and tone so we can attribute to an event the emotion behind it. For question 4 and 5 we will use mainly the gkg files as these files contain all the information that allows to answer our questions (locations, persons, emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_GCAM = gkg_df.select(\"GKGRECORDID\",\"GCAM\",\"Locations\",\"Persons\",\"V2Tone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ID_GCAM = ID_GCAM.toPandas()\n",
    "df_ID_GCAM.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> lets make sure we only have 1 ID per row (which is expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n', 'Length of all dataframe :' ,len(df_ID_GCAM), '\\n', \n",
    "            'Length of unique IDs    :' ,len(df_ID_GCAM['GKGRECORDID'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's focus on the emotions since the other columns are pretty straightforward. We need to split the emotions so then we can better attribute each event the correspondent feel as there are several dictionaries with different collection of words. For a more clear vision you can check [GCAM](https://blog.gdeltproject.org/introducing-the-global-content-analysis-measures-gcam/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the GCAM column\n",
    "df_ID_Emotions = df_ID_GCAM['GCAM'].str.split(',',expand=True)\n",
    "# insert the correspondent ID\n",
    "df_ID_Emotions.insert(loc=0, column='GKGRECORDID', value=df_ID_GCAM['GKGRECORDID'])\n",
    "df_ID_Emotions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4. b) Building the emotion dictionary <a id='Part2_Q4b'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build emotion dictionary\n",
    "Emotions_dictionary = get_emotion_dictionary(DATA_LOCAL, 'GCAM-MASTER-CODEBOOK.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets have a look at it\n",
    "Emotions_dictionary.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We see that different variables refer to different feelings.\n",
    "> After some research (as you'll see) one of the most common emotions is \"H4Lvd\" which clearly is not an emotion or a feeling of the speech, but it corresponds to 2 dictionaries. Meaning that these words belong to the Harvard and Lasswell dictionaries. In order to understand what this means, we went to see the spreadsheet of the words in these dictionaries, additional information found in [H4Lvd](http://www.wjh.harvard.edu/~inquirer/spreadsheet_guide.htm).\n",
    "When we don't have the specific emotion, it is useful to know which is the most common feeling associated with the most common dictionaries (therefore the check of the spreadsheets). For some others, we already know the feeling the dictionary refers to, such as \"Positivity\" via Lexicoder, “Smugness” via WordNet Affect, “Passivity” via Regressive Imagery Dictionary, etc. With this information we can associate these sentiments with the news and speeches for each event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 <a id='Part3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mainly do our plots with the plotly library. The choropleth world maps are drawn with GeoJSON (?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Q2_visualization import *\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q0. **Where does the news come from?** <a id='Part3_Q0'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the world map of the number of events grouped by country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the world map of the number of events reported by each country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. **Are we emotionally biased?** <a id='Part3_Q1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the average tone vs the distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. **Are some countries ignored in the news?** <a id='Part3_Q2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot a bubble plot, where the size of the bubble relates to the population count of the country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_q2_final = pd.read_csv(\"data_q2_final.csv\")\n",
    "visualization_q2(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. **Do we have a saturation limit?** / Q5. **Who is more emotional?** <a id='Part3_Q4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the top 10 most common emotions in 500 events (for each event)\n",
    "Top_Emotions_GCAM = dict_top_feelings(df_ID_Emotions,Emotions_dictionary,1,500)\n",
    "plot_commom_emotion(tuple_list = None, dictionary = Top_Emotions_GCAM, dictionary_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With this analysis we wanted to see if there are recurrent emotions in the news or speeches. Clearly there are. There might be some reasons to that, such as the dictionaries being so large that there are always a lot of words that belong to them or that in fact the words in those dictionaries are common. We will take this knowledge into account when attributing the emotions to countries or people (also because there are words that can belong to different dictionaries that have different meanings). For these feelings (when not obvious, such as \"POSITIVE\"), we went into extra efforts to understand what they mean."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
